{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"33fe0bc68bbb46a5ba09aa3523396c42","deepnote_cell_type":"markdown"},"source":"# TER - Extraction de texte et arXiv DOI Version Fonctionelle\n\n## Objectifs de ce Notebook?\n\nObjectifs:\n- parser un article scientifique au format PDF en texte\n- extraire de ce texte des informations utiles (articles cités, leur DOI arXiv, leurs auteurs)\n- générer le graphe de citation depuis l'article source\n- générer datasets avec contextes de citations \n\n## Données\n\nArticle source: /work/articletest.pdf\nCorpus test : /work/Test","block_group":"33fe0bc68bbb46a5ba09aa3523396c42"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5af7d00636d84af8a04006dcb9a9aaa3","deepnote_cell_type":"markdown"},"source":"### Prérequis:","block_group":"5af7d00636d84af8a04006dcb9a9aaa3"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1b03d628322b46f48937b7453fc8be6e","source_hash":"6a57895b","execution_start":1684220268688,"execution_millis":8361,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# coding=utf8\n\n!pip install PyPDF2==3.0.1 \n!pip install arxiv==1.4.3\n\nimport PyPDF2\nimport re\nimport pandas as pd\nimport arxiv","block_group":"1b03d628322b46f48937b7453fc8be6e","execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyPDF2==3.0.1 in /root/venv/lib/python3.9/site-packages (3.0.1)\nRequirement already satisfied: typing_extensions>=3.10.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from PyPDF2==3.0.1) (4.4.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: arxiv==1.4.3 in /root/venv/lib/python3.9/site-packages (1.4.3)\nRequirement already satisfied: feedparser in /root/venv/lib/python3.9/site-packages (from arxiv==1.4.3) (6.0.10)\nRequirement already satisfied: sgmllib3k in /root/venv/lib/python3.9/site-packages (from feedparser->arxiv==1.4.3) (1.0.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"fb1fa5f342e94639ad60c0b6e70f924e","deepnote_cell_type":"markdown"},"source":"#### Parse l'article en PDF vers TEXTE:","block_group":"fb1fa5f342e94639ad60c0b6e70f924e"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"68950ce930a84e5095f4d0466d169888","source_hash":"307f1b6c","execution_start":1684220277091,"execution_millis":1066,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"with open('articletest.pdf', 'rb') as file:\n    # créer un objet pdf\n    pdf = PyPDF2.PdfReader(file)\n     #extraire le texte de chaque page\n    t=''\n    for page in range (len(pdf.pages)): \n        t+=pdf.pages[page].extract_text()\n        #print(pdf.pages[page].extract_text())\n    print(t) #t stocke notre texte extrait du PDF","block_group":"68950ce930a84e5095f4d0466d169888","execution_count":2,"outputs":[{"name":"stdout","text":"consider ( salience ) is whether the linked document\ncan offer new, useful knowledge that may not be ob-\nvious to the current LM. Hyperlinks are potentially\nmore advantageous than lexical similarity links in\nthis regard: LMs are shown to be good at recogniz-\ning lexical similarity (Zhang et al., 2020), and hyper-\nlinks can bring in useful background knowledge thatmay not be obvious via lexical similarity alone (Asai\net al., 2020). Indeed, we empirically ﬁnd that using\nhyperlinks yields a more performant LM (§5.5).\nDiversity. In the document graph, some docu-\nments may have a very high in-degree (e.g., many\nincoming hyperlinks, like the “United States” page\nof Wikipedia), and others a low in-degree. If we uni-\nformly sample from the linked documents for each\nanchor segment, we may include documents of high\nin-degree too often in the overall training data, los-\ning diversity. To adjust so that all documents appear\nwith a similar frequency in training, we sample a\nlinked document with probability inversely propor-\ntional to its in-degree, as done in graph data mining\nliterature (Henzinger et al., 2000). We ﬁnd that this\ntechnique yields a better LM performance (§5.5).\n5 Experiments\nWe experiment with our proposed approach in the\ngeneral domain ﬁrst, where we pretrain LinkBERT\non Wikipedia articles with hyperlinks (§5.1) and\nevaluate on a suite of downstream tasks (§5.2). We\ncompare with BERT (Devlin et al., 2019) as our base-\nline. We experiment in the biomedical domain in §6.\n5.1 Pretraining setup\nData. We use the same pretraining corpus used\nby BERT: Wikipedia and BookCorpus (Zhu et al.,\n2015). For Wikipedia, we use the WikiExtractor3to\nextract hyperlinks between Wiki articles. We then\ncreate training instances by sampling contiguous ,\nrandom , orlinked segments as described in §4, with\nthe three options appearing uniformly (33%, 33%,\n33%). For BookCorpus, we create training instance\nby sampling contiguous orrandom segments (50%,\n50%) as in BERT. We then combine the training\ninstances from Wikipedia and BookCorpus to train\nLinkBERT. In summary, our pretraining data is\nthe same as BERT, except that we have hyperlinks\nbetween Wikipedia articles.\nImplementation. We pretrain LinkBERT of\nthree sizes, -tiny, -base and -large, following the\nconﬁgurations of BERT tiny (4.4M parameters),\nBERT base(110M params), and BERT large (340M\nparams) (Devlin et al., 2019; Turc et al., 2019). We\nuse -tiny mainly for ablation studies.\nFor -tiny, we pretrain from scratch with ran-\ndom weight initialization. We use the AdamW\n(Loshchilov and Hutter, 2019) optimizer with\n(\f1;\f2) = (0:9;0:98), warm up the learning rate\nfor the ﬁrst 5,000 steps and then linearly decay it.\n3https://github.com/attardi/wikiextractorWe train for 10,000 steps with a peak learning rate\n5e-3, weight decay 0.01, and batch size of 2,048\nsequences with 512 tokens. Training took 1 day on\ntwo GeForce RTX 2080 Ti GPUs with fp16.\nFor -base, we initialize LinkBERT with the\nBERT base checkpoint released by Devlin et al.\n(2019) and continue pretraining. We use a peak\nlearning rate 3e-4 and train for 40,000 steps. Other\ntraining hyperparameters are the same as -tiny.\nTraining took 4 days on four A100 GPUs with fp16.\nFor -large, we follow the same procedure as\n-base, except that we use a peak learning rate of 2e-4.\nTraining took 7 days on eight A100 GPUs with fp16.\nBaselines. We compare LinkBERT with BERT.\nSpeciﬁcally, for the -tiny scale, we compare with\nBERT tiny, which we pretrain from scratch with the\nsame hyperparameters as LinkBERT tiny. The only\ndifference is that LinkBERT uses document links\nto create LM inputs, while BERT does not.\nFor -base scale, we compare with BERT base, for\nwhich we take the BERT baserelease by Devlin et al.\n(2019) and continue pretraining it with the vanilla\nBERT objectives on the same corpus for the same\nnumber of steps as LinkBERT base.\nFor -large, we follow the same procedure as -base.\n5.2 Evaluation tasks\nWe ﬁne-tune and evaluate LinkBERT on a suite of\ndownstream tasks.\nExtractive question answering (QA). Given a\ndocument (or set of documents) and a question as\ninput, the task is to identify an answer span from\nthe document. We evaluate on six popular datasets\nfrom the MRQA shared task (Fisch et al., 2019):\nHotpotQA (Yang et al., 2018), TriviaQA (Joshi\net al., 2017), NaturalQ (Kwiatkowski et al., 2019),\nSearchQA (Dunn et al., 2017), NewsQA (Trischler\net al., 2017), and SQuAD (Rajpurkar et al., 2016).\nAs the MRQA shared task does not have a public\ntest set, we split the dev set in half to make new\ndev and test sets. We follow the ﬁne-tuning method\nBERT (Devlin et al., 2019) uses for extractive QA.\nMore details are provided in Appendix B.\nGLUE. The General Language Understanding\nEvaluation (GLUE) benchmark (Wang et al., 2018)\nis a popular suite of sentence-level classiﬁcation\ntasks. Following BERT, we evaluate on CoLA\n(Warstadt et al., 2019), SST-2 (Socher et al., 2013),\nMRPC (Dolan and Brockett, 2005), QQP ,STS-B\n(Cer et al., 2017), MNLI (Williams et al., 2017),\nQNLI (Rajpurkar et al., 2016), and RTE (Dagan\net al., 2005; Haim et al., 2006; GiampiccoloHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.\nBERT tiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0\nLinkBERT tiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1\nBERT base 76.0 70.3 74.2 76.5 65.7 88.7 75.2\nLinkBERT base 78.2 73.9 76.8 78.3 69.3 90.1 77.8\nBERT large 78.1 73.7 78.3 79.0 70.9 91.1 78.5\nLinkBERT large 80.8 78.2 80.5 81.0 72.6 92.7 81.0\nTable 1: Performance (F1) on MRQA question answering datasets. LinkBERT\nconsistently outperforms BERT on all datasets across the -tiny, -base, and -large scales.\nThe gain is especially large on datasets that require reasoning with multiple documents\nin the context, such as HotpotQA, TriviaQA, SearchQA.GLUE score\nBERT tiny 64.3\nLinkBERT tiny 64.6\nBERT base 79.2\nLinkBERT base 79.6\nBERT large 80.7\nLinkBERT large 81.1\nTable 2: Performance on the\nGLUE benchmark. LinkBERT\nattains comparable or moderately\nimproved performance.\nSQuAD SQuAD distract\nBERT base 88.7 85.9\nLinkBERT base 90.1 89.6\nTable 3: Performance (F1) on SQuAD when distracting\ndocuments are added to the context. While BERT incurs a\nlarge drop in F1, LinkBERT does not, suggesting its robustness\nin understanding document relations.\nHotpotQA TriviaQA NaturalQ SQuAD\nBERT base 64.8 59.2 64.8 79.6\nLinkBERT base 70.5 66.0 70.2 82.8\nTable 4: Few-shot QA performance (F1) when 10% of ﬁne-\ntuning data is used. LinkBERT attains large gains, suggesting\nthat it internalizes more knowledge than BERT in pretraining.\nHotpotQA TriviaQA NaturalQ SQuAD\nLinkBERT tiny 54.6 50.0 60.3 58.0\nNo diversity 53.5 48.0 60.0 57.8\nChange hyperlink to TF-IDF 50.0 48.2 59.6 57.6\nChange hyperlink to random 49.8 43.4 58.9 56.6\nTable 5: Ablation study on what linked documents to feed\ninto LM pretraining (§4.3).\nHotpotQA TriviaQA NaturalQ SQuADSQuAD\ndistract\nLinkBERT base 78.2 73.9 78.3 90.1 89.6\nNo DRP 76.5 72.5 77.0 89.3 87.0\nTable 6: Ablation study on the document relation prediction\n(DRP) objective in LM pretraining (§4.2).\net al., 2007), and report the average score. More\nﬁne-tuning details are provided in Appendix B.\n5.3 Results\nTable 1 shows the performance (F1 score) on\nMRQA datasets. LinkBERT substantially outper-\nforms BERT on all datasets. On average, the gain is\n+4.1% absolute for the BERT tinyscale, +2.6% for\nthe BERT basescale, and +2.5% for the BERT large\nscale. Table 2 shows the results on GLUE, where\nLinkBERT performs moderately better than BERT.\nThese results suggest that LinkBERT is especially\neffective at learning knowledge useful for QA tasks\n(e.g. world knowledge), while keeping performance\non sentence-level language understanding.\n5.4 Analysis\nWe further study when LinkBERT is especially\nuseful in downstream tasks.Improved multi-hop reasoning. In Table 1,\nwe ﬁnd that LinkBERT obtains notably large\ngains on QA datasets that require reasoning with\nmultiple documents, such as HotpotQA (+5% over\nBERT tiny), TriviaQA (+6%) and SearchQA (+8%),\nas opposed to SQuAD (+1.4%) which just has\na single document per question. To further gain\nqualitative insights, we studied in what QA exam-\nples LinkBERT succeeds but BERT fails. Figure\n3 shows a representative example from HotpotQA.\nAnswering the question needs 2-hop reasoning:\nidentify “Roden Brothers were taken over by Birks\nGroup” from the ﬁrst document, and then “Birks\nGroup is headquartered in Montreal” from the sec-\nond document. While BERT tends to simply predict\nan entity near the question entity (“Toronto” in the\nﬁrst document, which is just 1-hop), LinkBERT\ncorrectly predicts the answer in the second docu-\nment (“Montreal”). Our intuition is that because\nLinkBERT is pretrained with pairs of linked docu-\nments rather than purely single documents, it better\nlearns how to ﬂow information (e.g., do attention)\nacross tokens when multiple related documents\nare given in the context. In summary, these results\nsuggest that pretraining with linked documents\nhelps for multi-hop reasoning on downstream tasks.\nImproved understanding of document relations.\nWhile the MRQA datasets typically use ground-\ntruth documents as context for answering questions,\nin open-domain QA, QA systems need to use\ndocuments obtained by a retriever, which may\ninclude noisy documents besides gold ones (Chen\net al., 2017; Dunn et al., 2017). In such cases, QA\nsystems need to understand the document relations\nto perform well (Yang et al., 2018). To simulate\nthis setting, we modify the SQuAD dataset by\nprepending or appending 1–2 distracting documents\nto the original document given to each question.\nTable 3 shows the result. While BERT incurs a large\nperformance drop (-2.8%), LinkBERT is robust to\ndistracting documents (-0.5%). This result suggests\nthat pretraining with document links improves\nthe ability to understand document relations andThree days after undergoing a laparoscopic Whipple's procedure, a 43-year-old woman has swelling of her right leg. ... She was diagnosed with pancreatic cancer 1 month ago. ... Her temperature is 38°C (100.4°F), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination shows mild swelling of the right thigh to the ankle; there is no erythema or pitting edema. ... Which of the following is the most appropriate next step in management?(A)  CT pulmonary angiography     (B)  Compression ultrasonography(C)  D-dimer level                                 (D)  2 sets of blood culturesLinkBERT predicts: B (✓)    PubmedBERT predicts: D (✗)Leg swelling, pancreatic cancer(symptom) Deep vein thrombosis(possible cause)Compression ultrasonography(next step for diagnosis)Doc A: ... Pancreatic cancer can induce deep vein thrombosis in leg ...      (e.g. Ansari et al. 2015)Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... (e.g. Piovella et al. 2002)\n[Tidal Basin, Washington D.C.]The Tidal Basin is a man-made reservoir located between the Potomac River and the Washington Channel in Washington, D.C. It is part of West Potomac Park, is near the National Mall and is a focal point of the National Cherry Blossom Festival held each spring. The Jeﬀerson Memorial, the Martin Luther King Jr. Memorial, the Franklin Delano Roosevelt Memorial, and the George Mason Memorial are situated adjacent to the Tidal Basin. MedQA-USMLE exampleNeed multi-hop reasoning\n[The National Cherry Blossom Festival] … It is a spring celebration commemorating the March 27, 1912, gift of Japanese cherry trees from Mayor of Tokyo City to the city of Washington, D.C. ... Of the initial gift of 12 varieties of 3,020 trees, the Yoshino Cherry now dominates. ...Knowledge learned via document linksReference\nQuestion: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \"Rich Cut Glass\" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three diﬀerent retail banners: … The company is headquartered in Montreal, Quebec, with American corporate oﬀices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)                                 BERT prediction: “Toronto” (✗)HotpotQA exampleQuestion: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \"Rich Cut Glass\" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three different retail banners: ... The company is headquartered in Montreal, Quebec, with American corporate offices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)     BERT prediction: “Toronto” (✗)HotpotQA example\nLinkBERT predicts: “Montreal” (✓)      BERT predicts: “Toronto” (✗)Figure 3: Case study of multi-hop reasoning on HotpotQA.\nAnswering the question needs to identify “Roden Brothers\nwere taken over by Birks Group” from the ﬁrst document,\nand then “Birks Group is headquartered in Montreal” from\nthe second document. While BERT tends to simply predict\nan entity near the question entity (“Toronto” in the ﬁrst\ndocument), LinkBERT correctly predicts the answer in the\nsecond document (“Montreal”).\nrelevance. In particular, our intuition is that the\nDRP objective helps the LM to better recognize\ndocument relations like (anchor document, linked\ndocument) in pretraining, which helps to recognize\nrelations like (question, right document) in down-\nstream QA tasks. We indeed ﬁnd that ablating the\nDRP objective from LinkBERT hurts performance\n(§5.5). The strength of understanding document\nrelations also suggests the promise of applying\nLinkBERT to various retrieval-augmented methods\nand tasks (e.g. Lewis et al. 2020b), either as the\nmain LM or the dense retriever component.\nImproved few-shot QA performance. We also\nﬁnd that LinkBERT is notably good at few-shot\nlearning. Concretely, for each MRQA dataset, we\nﬁne-tune with only 10% of the available training\ndata, and report the performance in Table 4. In this\nfew-shot regime, LinkBERT attains more signiﬁ-\ncant gains over BERT, compared to the full-resource\nregime in Table 1 (on NaturalQ, 5.4% vs 1.8% abso-\nlute in F1, or 15% vs 7% in relative error reduction).\nThis result suggests that LinkBERT internalizes\nmore knowledge than BERT during pretraining,\nwhich supports our core idea that document links\ncan bring in new, useful knowledge for LMs.\n5.5 Ablation studies\nWe conduct ablation studies on the key design\nchoices of LinkBERT.\nWhat linked documents to feed into LMs? We\nstudy the strategies discussed in §4.3 for obtaining\nlinked documents: relevance, salience, and diversity.Table 5 shows the ablation result on MRQA datasets.\nFirst, if we ignore relevance and use random doc-\nument links instead of hyperlinks, we get the same\nperformance as BERT (-4.1% on average; “random”\nin Table 5). Second, using lexical similarity links\ninstead of hyperlinks leads to 1.8% performance\ndrop (“TF-IDF”). Our intuition is that hyperlinks\ncan provide more salient knowledge that may not be\nobvious from lexical similarity alone. Nevertheless,\nusing lexical similarity links is substantially better\nthan BERT (+2.3%), conﬁrming the efﬁcacy of\nplacing relevant documents together in the input\nfor LM pretraining. Finally, removing the diversity\nadjustment in document sampling leads to 1% per-\nformance drop (“No diversity”). In summary, our\ninsight is that to create informative inputs for LM\npretraining, the linked documents must be seman-\ntically relevant and ideally be salient and diverse.\nEffect of the DRP objective. Table 6 shows the\nablation result on the DRP objective (§4.2). Re-\nmoving DRP in pretraining hurts downstream QA\nperformance. The drop is large on tasks with multi-\nple documents (HotpotQA, TriviaQA, and SQuAD\nwith distracting documents). This suggests that\nDRP facilitates LMs to learn document relations.\n6 Biomedical LinkBERT ( BioLinkBERT )\nPretraining LMs on biomedical text is shown\nto boost performance on biomedical NLP tasks\n(Beltagy et al., 2019; Lee et al., 2020; Lewis\net al., 2020a; Gu et al., 2020). Biomedical LMs\nare typically trained on PubMed, which contains\nabstracts and citations of biomedical papers. While\nprior works only use their raw text for pretraining,\nacademic papers have rich dependencies with each\nother via citations (references). We hypothesize\nthat incorporating citation links can help LMs learn\ndependencies between papers and knowledge that\nspans across them.\nWith this motivation, we pretrain LinkBERT on\nPubMed with citation links (§6.1), which we term\nBioLinkBERT , and evaluate on biomedical down-\nstream tasks (§6.2). As our baseline, we follow and\ncompare with the state-of-the-art biomedical LM,\nPubmedBERT (Gu et al., 2020), which has the same\narchitecture as BERT and is trained on PubMed.\n6.1 Pretraining setup\nData. We use the same pretraining corpus used\nby PubmedBERT: PubMed abstracts (21GB).4We\n4https://pubmed.ncbi.nlm.nih.gov . We use papers\npublished before Feb. 2020 as in PubmedBERT.use the Pubmed Parser5to extract citation links be-\ntween articles. We then create training instances by\nsampling contiguous ,random , orlinked segments\nas described in §4, with the three options appearing\nuniformly (33%, 33%, 33%). In summary, our pre-\ntraining data is the same as PubmedBERT, except\nthat we have citation links between PubMed articles.\nImplementation. We pretrain BioLinkBERT of\n-base size (110M params) from scratch, following\nthe same hyperparamters as the PubmedBERT base\n(Gu et al., 2020). Speciﬁcally, we use a peak\nlearning rate 6e-4, batch size 8,192, and train for\n62,500 steps. We warm up the learning rate in\nthe ﬁrst 10% of steps and then linearly decay it.\nTraining took 7 days on eight A100 GPUs with fp16.\nAdditionally, while the original PubmedBERT\nrelease did not include the -large size, we pretrain\nBioLinkBERT of the -large size (340M params)\nfrom scratch, following the same procedure as\n-base, except that we use a peak learning rate of 4e-4\nand warm up steps of 20%. Training took 21 days\non eight A100 GPUs with fp16.\nBaselines. We compare BioLinkBERT with\nPubmedBERT released by Gu et al. (2020).\n6.2 Evaluation tasks\nFor downstream tasks, we evaluate on the BLURB\nbenchmark (Gu et al., 2020), a diverse set of biomed-\nical NLP datasets, and MedQA-USMLE (Jin et al.,\n2021), a challenging biomedical QA dataset.\nBLURB consists of ﬁve named entity recog-\nnition tasks, a PICO (population, intervention,\ncomparison, and outcome) extraction task, three\nrelation extraction tasks, a sentence similarity task,\na document classiﬁcation task, and two question\nanswering tasks, as summarized in Table 7. We\nfollow the same ﬁne-tuning method and evaluation\nmetric used by PubmedBERT (Gu et al., 2020).\nMedQA-USMLE is a 4-way multi-choice QA\ntask that tests biomedical and clinical knowledge.\nThe questions are from practice tests for the US\nMedical License Exams (USMLE). The questions\ntypically require multi-hop reasoning, e.g., given\npatient symptoms, infer the likely cause, and then\nanswer the appropriate diagnosis procedure (Figure\n4). We follow the ﬁne-tuning method in Jin et al.\n(2021). More details are provided in Appendix B.\nMMLU-professional medicine is a multi-\nchoice QA task that tests biomedical knowledge\nand reasoning, and is part of the popular MMLU\n5https://github.com/titipata/pubmed_parserPubMed-\nBERT baseBioLink-\nBERT baseBioLink-\nBERT large\nNamed entity recognition\nBC5-chem (Li et al., 2016) 93.33 93.75 94.04\nBC5-disease (Li et al., 2016) 85.62 86.10 86.39\nNCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76\nBC2GM (Smith et al., 2008) 84.52 84.90 85.18\nJNLPBA (Kim et al., 2004) 80.06 79.03 80.06\nPICO extraction\nEBM PICO (Nye et al., 2018) 73.38 73.97 74.19\nRelation extraction\nChemProt (Krallinger et al., 2017) 77.24 77.57 79.98\nDDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35\nGAD (Bravo et al., 2015) 82.34 84.39 84.90\nSentence similarity\nBIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63\nDocument classiﬁcation\nHoC (Baker et al., 2016) 82.32 84.35 84.87\nQuestion answering\nPubMedQA (Jin et al., 2019) 55.84 70.20 72.18\nBioASQ (Nentidis et al., 2019) 87.56 91.43 94.82\nBLURB score 81.10 83.39 84.30\nTable 7: Performance on BLURB benchmark. BioLinkBERT\nattains improvement on all tasks, establishing new state of\nthe art on BLURB. Gains are notably large on document-level\ntasks such as PubMedQA and BioASQ.\nMethods Acc. (%)\nBioBERT large (Lee et al., 2020) 36.7\nQAGNN (Yasunaga et al., 2021) 38.0\nGreaseLM (Zhang et al., 2022) 38.5\nPubmedBERT base (Gu et al., 2020) 38.1\nBioLinkBERT base(Ours ) 40.0\nBioLinkBERT large (Ours ) 44.6\nTable 8: Performance on MedQA-USMLE. BioLinkBERT\noutperforms all previous biomedical LMs.\nMethods Acc. (%)\nGPT-3 (175B params) (Brown et al., 2020) 38.7\nUniﬁedQA (11B params) (Khashabi et al., 2020) 43.2\nBioLinkBERT large (Ours ) 50.7\nTable 9: Performance on MMLU-professional medicine.\nBioLinkBERT signiﬁcantly outperforms the largest general-\ndomain LM or QA model, despite having just 340M parameters.\nbenchmark (Hendrycks et al., 2021) that is used\nto evaluate massive language models. We take the\nBioLinkBERT ﬁne-tuned on the above MedQA-\nUSMLE task, and evaluate on this task without\nfurther adaptation.\n6.3 Results\nBLURB. Table 7 shows the results on BLURB.\nBioLinkBERT baseoutperforms PubmedBERT base\non all task categories, attaining a performance\nboost of +2% absolute on average. Moreover,\nBioLinkBERT large provides a further boost of\n+1%. In total, BioLinkBERT outperforms the\nprevious best by +3% absolute, establishing a new\nstate of the art on the BLURB leaderboard. We see a\ntrend that gains are notably large on document-levelThree days after undergoing a laparoscopic Whipple's procedure, a 43-year-old woman has swelling of her right leg. ... She was diagnosed with pancreatic cancer 1 month ago. ... Her temperature is 38°C (100.4°F), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination shows mild swelling of the right thigh to the ankle; there is no erythema or pitting edema. ... Which of the following is the most appropriate next step in management?(A)  CT pulmonary angiography     (B)  Compression ultrasonography(C)  D-dimer level                                 (D)  2 sets of blood culturesLinkBERT predicts: B (✓)    PubmedBERT predicts: D (✗)Leg swelling, pancreatic cancer(symptom) Deep vein thrombosis(possible cause)Compression ultrasonography(next step for diagnosis)Doc A: ... Pancreatic cancer can induce deep vein thrombosis in leg ...      (e.g. Ansari et al. 2015)Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... (e.g. Piovella et al. 2002)\n[Tidal Basin, Washington D.C.]The Tidal Basin is a man-made reservoir located between the Potomac River and the Washington Channel in Washington, D.C. It is part of West Potomac Park, is near the National Mall and is a focal point of the National Cherry Blossom Festival held each spring. The Jeﬀerson Memorial, the Martin Luther King Jr. Memorial, the Franklin Delano Roosevelt Memorial, and the George Mason Memorial are situated adjacent to the Tidal Basin. MedQA-USMLE exampleNeed multi-hop reasoning\n[The National Cherry Blossom Festival] … It is a spring celebration commemorating the March 27, 1912, gift of Japanese cherry trees from Mayor of Tokyo City to the city of Washington, D.C. ... Of the initial gift of 12 varieties of 3,020 trees, the Yoshino Cherry now dominates. ...Knowledge learned via document linksReference\nQuestion: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \"Rich Cut Glass\" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three diﬀerent retail banners: … The company is headquartered in Montreal, Quebec, with American corporate oﬀices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)                                 BERT prediction: “Toronto” (✗)HotpotQA exampleQuestion: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, \"Rich Cut Glass\" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three different retail banners: ... The company is headquartered in Montreal, Quebec, with American corporate offices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)     BERT prediction: “Toronto” (✗)HotpotQA example\nLinkBERT predicts: “Montreal” (✓)       BERT predicts: “Toronto” (✗)Figure 4: Case study of multi-hop reasoning on MedQA-USMLE. Answering the question (left) needs 2-hop reasoning (center):\nfrom the patient symptoms described in the question ( leg swelling ,pancreatic cancer ), infer the cause ( deep vein thrombosis ),\nand then infer the appropriate diagnosis procedure ( compression ultrasonography ). While the existing PubmedBERT tends to\nsimply predict a choice that contains a word appearing in the question (“blood” for choice D), BioLinkBERT correctly predicts\nthe answer (B). Our intuition is that citation links bring relevant documents together in the same context in pretraining (right),\nwhich readily provides the multi-hop knowledge needed for the reasoning (center).\ntasks such as question answering (+7% on BioASQ\nand PubMedQA). This result is consistent with the\ngeneral domain (§5.3) and conﬁrms that LinkBERT\nhelps to learn document dependencies better.\nMedQA-USMLE. Table 8 shows the results.\nBioLinkBERT base obtains a 2% accuracy boost\nover PubmedBERT base, and BioLinkBERT large\nprovides an additional +5% boost. In total, Bi-\noLinkBERT outperforms the previous best by +7%\nabsolute, setting a new state of the art. To further\ngain qualitative insights, we studied in what QA\nexamples BioLinkBERT succeeds but the baseline\nPubmedBERT fails. Figure 4 shows a representative\nexample. Answering the question (left) needs 2-hop\nreasoning (center): from the patient symptoms\ndescribed in the question ( leg swelling ,pancreatic\ncancer ), infer the cause ( deep vein thrombosis ),\nand then infer the appropriate diagnosis procedure\n(compression ultrasonography ). We ﬁnd that while\nthe existing PubmedBERT tends to simply predict\na choice that contains a word appearing in the\nquestion (“blood” for choice D), BioLinkBERT\ncorrectly predicts the answer (B). Our intuition is\nthat citation links bring relevant documents and\nconcepts together in the same context in pretraining\n(right),6which readily provides the multi-hop\nknowledge needed for the reasoning (center). Com-\nbined with the analysis on HotpotQA (§5.4), our\nresults suggest that pretraining with document links\nconsistently helps for multi-hop reasoning across\ndomains (e.g., general documents with hyperlinks\nand biomedical articles with citation links).\n6For instance, as in Figure 4 (right), Ansari et al. (2015) in\nPubMed mention that pancreatic cancer can induce deep vein\nthrombosis in leg , and it cites a paper in PubMed, Piovella et al.\n(2002), which mention that deep vein thrombosis is tested by\ncompression ultrasonography . Placing these two documents\nin the same context yields the complete multi-hop knowledge\nneeded to answer the question (“ pancreatic cancer ”!“deep\nvein thrombosis ”!“compression ultrasonography ”).MMLU-professional medicine. Table 9 shows\nthe performance. Despite having just 340M parame-\nters, BioLinkBERT large achieves 50% accuracy on\nthis QA task, signiﬁcantly outperforming the largest\ngeneral-domain LM or QA models such as GPT-3\n175B params (39% accuracy) and UniﬁedQA 11B\nparams (43% accuracy). This result shows that\nwith an effective pretraining approach, a small\ndomain-specialized LM can outperform orders of\nmagnitude larger language models on QA tasks.\n7 Conclusion\nWe presented LinkBERT, a new language model\n(LM) pretraining method that incorporates docu-\nment link knowledge such as hyperlinks. In both\nthe general domain (pretrained on Wikipedia with\nhyperlinks) and biomedical domain (pretrained on\nPubMed with citation links), LinkBERT outper-\nforms previous BERT models across a wide range\nof downstream tasks. The gains are notably large\nfor multi-hop reasoning, multi-document under-\nstanding and few-shot question answering, suggest-\ning that LinkBERT effectively internalizes salient\nknowledge through document links. Our results sug-\ngest that LinkBERT can be a strong pretrained LM\nto be applied to various knowledge-intensive tasks.\nReproducibility\nPretrained models, code and data are available at\nhttps://github.com/michiyasunaga/\nLinkBERT .\nExperiments are available at\nhttps://worksheets.\ncodalab.org/worksheets/\n0x7a6ab9c8d06a41d191335b270da2902e .\nAcknowledgment\nWe thank Siddharth Karamcheti, members of the\nStanford P-Lambda, SNAP and NLP groups, as\nwell as our anonymous reviewers for valuablefeedback. We gratefully acknowledge the sup-\nport of a PECASE Award; DARPA under Nos.\nHR00112190039 (TAMI), N660011924033 (MCS);\nFunai Foundation Fellowship; Microsoft Research\nPhD Fellowship; ARO under Nos. W911NF-16-1-\n0342 (MURI), W911NF-16-1-0171 (DURIP); NSF\nunder Nos. OAC-1835598 (CINES), OAC-1934578\n(HDR), CCF-1918940 (Expeditions), IIS-2030477\n(RAPID), NIH under No. R56LM013365; Stanford\nData Science Initiative, Wu Tsai Neurosciences\nInstitute, Chan Zuckerberg Biohub, Amazon,\nJPMorgan Chase, Docomo, Hitachi, Intel, KDDI,\nToshiba, NEC, Juniper, and UnitedHealth Group.\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. 2021. Htlm: Hyper-text pre-training\nand prompting of language models. arXiv preprint\narXiv:2107.06955 .\nDavid Ansari, Daniel Ansari, Roland Andersson, and\nÅke Andrén-Sandberg. 2015. Pancreatic cancer and\nthromboembolic disease, 150 years after trousseau.\nHepatobiliary surgery and nutrition , 4(5):325.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning\nto retrieve reasoning paths over wikipedia graph for\nquestion answering. In International Conference on\nLearning Representations (ICLR) .\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali,\nJohan Högberg, Ulla Stenius, and Anna Korhonen.\n2016. Automatic semantic classiﬁcation of scientiﬁc\nliterature according to the hallmarks of cancer.\nBioinformatics .\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nPretrained language model for scientiﬁc text. In\nEmpirical Methods in Natural Language Processing\n(EMNLP) .\nChandra Bhagavatula, Sergey Feldman, Russell Power,\nand Waleed Ammar. 2018. Content-based citation\nrecommendation. In North American Chapter of the\nAssociation for Computational Linguistics (NAACL) .\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, SaahilJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak,\nMina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik,\nChristopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika\nNarayan, Deepak Narayanan, Ben Newman, Allen\nNie, Juan Carlos Niebles, Hamed Nilforoshan, Julian\nNyarko, Giray Ogut, Laurel Orr, Isabel Papadim-\nitriou, Joon Sung Park, Chris Piech, Eva Portelance,\nChristopher Potts, Aditi Raghunathan, Rob Reich,\nHongyu Ren, Frieda Rong, Yusuf Roohani, Camilo\nRuiz, Jack Ryan, Christopher Ré, Dorsa Sadigh,\nShiori Sagawa, Keshav Santhanam, Andy Shih,\nKrishnan Srinivasan, Alex Tamkin, Rohan Taori,\nArmin W. Thomas, Florian Tramèr, Rose E. Wang,\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu,\nSang Michael Xie, Michihiro Yasunaga, Jiaxuan\nYou, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn\nZhou, and Percy Liang. 2021. On the opportunities\nand risks of foundation models. arXiv preprint\narXiv:2108.07258 .\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems (NeurIPS) .\nAntoine Bosselut, Hannah Rashkin, Maarten Sap,\nChaitanya Malaviya, Asli Çelikyilmaz, and Yejin\nChoi. 2019. Comet: Commonsense transformers\nfor automatic knowledge graph construction. In\nAssociation for Computational Linguistics (ACL) .\nÀlex Bravo, Janet Piñero, Núria Queralt-Rosinach,\nMichael Rautschka, and Laura I Furlong. 2015.\nExtraction of relations between genes and diseases\nfrom text and large-scale data analysis: implications\nfor translational research. BMC bioinformatics .\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-\nshot learners. In Advances in Neural Information\nProcessing Systems (NeurIPS) .\nAvi Caciularu, Arman Cohan, Iz Beltagy, Matthew E\nPeters, Arie Cattan, and Ido Dagan. 2021. Cross-\ndocument language modeling. arXiv preprint\narXiv:2101.00406 .\nIacer Calixto, Alessandro Raganato, and Tommaso\nPasini. 2021. Wikipedia entities as rendezvous\nacross languages: Grounding multilingual language\nmodels by predicting wikipedia hyperlinks. In\nNorth American Chapter of the Association for\nComputational Linguistics (NAACL) .\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017task 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. In International\nWorkshop on Semantic Evaluation (SemEval) .\nWei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2020. Pre-training tasks\nfor embedding-based large-scale retrieval. In Inter-\nnational Conference on Learning Representations\n(ICLR) .\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Association for Computational\nLinguistics (ACL) .\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S Weld. 2020. Specter:\nDocument-level representation learning using\ncitation-informed transformers. In Association for\nComputational Linguistics (ACL) .\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In North American Chapter of the Association\nfor Computational Linguistics (NAACL) .\nRezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for\ndisease name recognition and concept normalization.\nJournal of biomedical informatics .\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur\nGuney, V olkan Cirik, and Kyunghyun Cho. 2017.\nSearchqa: A new q&a dataset augmented with\ncontext from a search engine. arXiv preprint\narXiv:1704.05179 .\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,\nEunsol Choi, and Danqi Chen. 2019. Mrqa 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Workshop on Machine Reading\nfor Question Answering .\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-\nshot learners. In Association for Computational\nLinguistics (ACL) .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand William B Dolan. 2007. The third pascal recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing .Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2020. Domain-speciﬁc lan-\nguage model pretraining for biomedical natural lan-\nguage processing. arXiv preprint arXiv:2007.15779 .\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In Interna-\ntional Conference on Machine Learning (ICML) .\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpek-\ntor. 2006. The second pascal recognising textual\nentailment challenge. In Proceedings of the Second\nPASCAL Challenges Workshop on Recognising\nTextual Entailment .\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020. Integrating\ngraph contextualized knowledge into pre-trained\nlanguage models. In Findings of EMNLP .\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. In International Conference on\nLearning Representations (ICLR) .\nMonika R Henzinger, Allan Heydon, Michael Mitzen-\nmacher, and Marc Najork. 2000. On near-uniform\nurl sampling. Computer Networks .\nMaría Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMartínez, and Thierry Declerck. 2013. The ddi\ncorpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions. Journal of\nbiomedical informatics .\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka\nZitnik, Percy Liang, Vijay Pande, and Jure Leskovec.\n2020. Strategies for pre-training graph neural\nnetworks. In International Conference on Learning\nRepresentations (ICLR) .\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams.\nApplied Sciences .\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In\nEmpirical Methods in Natural Language Processing\n(EMNLP) .\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nbert: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics .\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehen-\nsion. In Association for Computational Linguistics\n(ACL) .\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval\nfor open-domain question answering. In Empirical\nMethods in Natural Language Processing (EMNLP) .\nAnita Khadka, Ivan Cantador, and Miriam Fernandez.\n2020. Exploiting citation knowledge in personalised\nrecommendation of recent scientiﬁc publications.\nInLanguage Resources and Evaluation Conference\n(LREC) .\nDaniel Khashabi, Tushar Khot, Ashish Sabharwal,\nOyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Uniﬁedqa: Crossing format bound-\naries with a single qa system. In Findings of EMNLP .\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,\nYuka Tateisi, and Nigel Collier. 2004. Introduction\nto the bio-entity recognition task at jnlpba. In\nProceedings of the international joint workshop on\nnatural language processing in biomedicine and its\napplications .\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\nMartın Pérez Pérez, Jesús Santamaría, Gael Pérez\nRodríguez, Georgios Tsatsaronis, and Ander In-\ntxaurrondo. 2017. Overview of the biocreative vi\nchemical-protein interaction track. In Proceed-\nings of the sixth BioCreative challenge evaluation\nworkshop .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics\n(TACL) .\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics .\nYoav Levine, Noam Wies, Daniel Jannai, Dan Navon,\nYedid Hoshen, and Amnon Shashua. 2021. The\ninductive bias of in-context learning: Rethink-\ning pretraining example design. arXiv preprint\narXiv:2110.04541 .\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin\nStoyanov. 2020a. Pretrained language models for\nbiomedical and clinical tasks: Understanding and ex-\ntending the state-of-the-art. In Proceedings of the 3rd\nClinical Natural Language Processing Workshop .\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, et al. 2020b. Retrieval-augmentedgeneration for knowledge-intensive nlp tasks. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) .\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky,\nChih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase .\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 .\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International\nConference on Learning Representations (ICLR) .\nZhengyi Ma, Zhicheng Dou, Wei Xu, Xinyu Zhang,\nHao Jiang, Zhao Cao, and Ji-Rong Wen. 2021. Pre-\ntraining for ad-hoc retrieval: Hyperlink is also you\nneed. In Conference on Information and Knowledge\nManagement (CIKM) .\nEric Margolis, Stephen Laurence, et al. 1999. Concepts:\ncore readings . Mit Press.\nAnastasios Nentidis, Konstantinos Bougiatiotis, Anas-\ntasia Krithara, and Georgios Paliouras. 2019. Results\nof the seventh edition of the bioasq challenge. In\nJoint European Conference on Machine Learning\nand Knowledge Discovery in Databases .\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei\nYang, Iain J Marshall, Ani Nenkova, and Byron C\nWallace. 2018. A corpus with multi-level anno-\ntations of patients, interventions and outcomes to\nsupport language processing for medical literature.\nInProceedings of the conference. Association for\nComputational Linguistics. Meeting .\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\nStan Peshterliev, Dmytro Okhonko, Michael\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\nScott Yih. 2020. Uniﬁed open-domain question an-\nswering with structured and unstructured knowledge.\narXiv preprint arXiv:2012.14610 .\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as\nknowledge bases? In Empirical Methods in Natural\nLanguage Processing (EMNLP) .\nFranco Piovella, Luciano Crippa, Marisa Barone,\nS Vigano D’Angelo, Silvia Seraﬁni, Laura Galli,\nChiara Beltrametti, and Armando D’Angelo. 2002.\nNormalization rates of compression ultrasonogra-\nphy in patients with a ﬁrst episode of deep vein\nthrombosis of the lower limbs: association with\nrecurrence and new thrombosis. Haematologica ,\n87(5):515–522.Vahed Qazvinian and Dragomir R Radev. 2008.\nScientiﬁc paper summarization using citation sum-\nmary networks. In International Conference on\nComputational Linguistics (COLING) .\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR) .\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. In Empirical\nMethods in Natural Language Processing (EMNLP) .\nYeon Seonwoo, Sang-Woo Lee, Ji-Hoon Kim, Jung-\nWoo Ha, and Alice Oh. 2021. Weakly supervised pre-\ntraining for multi-hop retriever. In Findings of ACL .\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2020. Towards controllable biases\nin language generation. In the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP)-Findings, long .\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu,\nYu-Shi Lin, Roman Klinger, Christoph M Friedrich,\nKuzman Ganchev, et al. 2008. Overview of biocre-\native ii gene mention recognition. Genome biology .\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP) .\nGizem So ˘gancıo ˘glu, Hakime Öztürk, and Arzucan\nÖzgür. 2017. Biosses: a semantic sentence simi-\nlarity estimation system for the biomedical domain.\nBioinformatics .\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng\nGuo, Yaru Hu, Xuan-Jing Huang, and Zheng Zhang.\n2020. Colake: Contextualized language and knowl-\nedge embedding. In International Conference on\nComputational Linguistics (COLING) .\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin\nHarris, Alessandro Sordoni, Philip Bachman, and\nKaheer Suleman. 2017. Newsqa: A machine com-\nprehension dataset. In Workshop on Representation\nLearning for NLP .\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962 .\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In EMNLPWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP .\nHongwei Wang, Hongyu Ren, and Jure Leskovec.\n2021a. Relational message passing for knowledge\ngraph completion. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery &\nData Mining .\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. Kepler: A uniﬁed model for knowledge\nembedding and pre-trained language representation.\nTransactions of the Association for Computational\nLinguistics .\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics .\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNorth American Chapter of the Association for\nComputational Linguistics (NAACL) .\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. Uniﬁedskg:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. arXiv\npreprint arXiv:2201.05966 .\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In International Conference on Learning\nRepresentations (ICLR) .\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Empirical Methods in Natural Language\nProcessing (EMNLP) .\nMichihiro Yasunaga, Jungo Kasai, Rui Zhang,\nAlexander R Fabbri, Irene Li, Dan Friedman, and\nDragomir R Radev. 2019. Scisummnet: A large\nannotated corpus and content-impact models for sci-\nentiﬁc paper summarization with citation networks.\nInProceedings of the AAAI Conference on Artiﬁcial\nIntelligence .\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nReasoning with language models and knowledge\ngraphs for question answering. In North American\nChapter of the Association for Computational\nLinguistics (NAACL) .Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu,\nAyush Pareek, Krishnan Srinivasan, and Dragomir\nRadev. 2017. Graph-based neural multi-document\nsummarization. In Conference on Computational\nNatural Language Learning (CoNLL) .\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations (ICLR) .\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\nHongyu Ren, Percy Liang, Christopher D Man-\nning, and Jure Leskovec. 2022. Greaselm: Graph\nreasoning enhanced language models for question\nanswering. In International Conference on Learning\nRepresentations (ICLR) .\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities.\narXiv preprint arXiv:1905.07129 .\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In International Conference on\nComputer Vision (ICCV) .\nA Ethics, limitations and risks\nWe outline potential ethical issues with our work\nbelow. First, LinkBERT is trained on the same\ntext corpora (e.g., Wikipedia, Books, PubMed)\nas in existing language models. Consequently,\nLinkBERT could reﬂect the same biases and toxic\nbehaviors exhibited by language models, such as\nbiases about race, gender, and other demographic\nattributes (Sheng et al., 2020).\nAnother source of ethical concern is the use of\nthe MedQA-USMLE evaluation (Jin et al., 2021).\nWhile we ﬁnd this clinical reasoning task to be an\ninteresting testbed for LinkBERT and for multi-hop\nreasoning in general, we do not encourage users\nto use the current models for real world clinical\nprediction.\nB Fine-tuning details\nWe apply the following ﬁne-tuning hyperparameters\nto all models, including the baselines.\nMRQA. For all the extractive question answering\ndatasets, we use max_seq_length = 384 and a\nsliding window of size 128if the lengths are longer\nthanmax_seq_length .\nFor the -tiny scale (BERT tiny, LinkBERT tiny),\nwe choose learning rates from {5e-5, 1e-4, 3e-4},\nbatch sizes from {16, 32, 64}, and ﬁne-tuning\nepochs from {5, 10}.For -base (BERT base, LinkBERT base), we\nchoose learning rates from {2e-5, 3e-5}, batch sizes\nfrom {12, 24}, and ﬁne-tuning epochs from {2, 4}.\nFor -large (BERT large , LinkBERT large ), we\nchoose learning rates from {1e-5, 2e-5}, batch sizes\nfrom {16, 32}, and ﬁne-tuning epochs from {2, 4}.\nGLUE. We use max_seq_length = 128.\nFor the -tiny scale (BERT tiny, LinkBERT tiny),\nwe choose learning rates from {5e-5, 1e-4, 3e-4},\nbatch sizes from {16, 32, 64}, and ﬁne-tuning\nepochs from {5, 10}.\nFor -base and -large (BERT base, LinkBERT base,\nBERT large , LinkBERT large ), we choose learning\nrates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5}, batch sizes\nfrom {16, 32, 64} and ﬁne-tuning epochs from 3–10.\nBLURB. We use max_seq_length = 512 and\nchoose learning rates from {1e-5, 2e-5, 3e-5, 5e-5,\n6e-5}, batch sizes from {16, 32, 64} and ﬁne-tuning\nepochs from 1–120.\nMedQA-USMLE. We use max_seq_length =\n512 and choose learning rates from {1e-5, 2e-5,\n3e-5}, batch sizes from {16, 32, 64} and ﬁne-tuning\nepochs from 1–6.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c3bff1437d0349abb3ea1fd568941471","deepnote_cell_type":"markdown"},"source":"#### On match les auteurs dans l'article pour ensuite les matcher avec les arXiv_ID présent dans les references:","block_group":"c3bff1437d0349abb3ea1fd568941471"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9e298b164a4e43c4b513aab6cddf1ad4","source_hash":"eb1785c4","execution_start":1684220278169,"execution_millis":325,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"new_df = pd.DataFrame(columns=['Author'])\n\n\nregex = r\"((?:[A-Z][A-Za-z'`-]+\\s*et al.*\\s*.\\d{4}))|((?:[A-Z][a-z]*\\s+)(?:[a-z]*\\s*)(?:[A-Z][a-z]*)(?:,\\s*)(?:\\d{4}))|(?:[A-Z][a-z]*\\s+)(?:et al.?)(?:,\\s)(?:\\d{4})|((?:[A-Z][a-z]*\\s+)(?:al.?)(?:,\\s*)(?:\\d{4}))\"\n\ntest_str = t\n\nmatches = re.finditer(regex, test_str, re.MULTILINE)\n\nfor matchNum, match in enumerate(matches, start=1):\n    \n    row = [match.group()]\n    new_df.loc[len(new_df)] = row\n\n#new_df","block_group":"9e298b164a4e43c4b513aab6cddf1ad4","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"4e140f542f8444bf9165e49c295011fe","deepnote_cell_type":"markdown"},"source":"#### Formatage du DataFrame précedent:","block_group":"4e140f542f8444bf9165e49c295011fe"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"cb21fd11b9a843eb82a53f505c0edb04","source_hash":"2592e0c","execution_start":1684220278513,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"new_df = new_df['Author'].str.split(';', expand=True).stack().reset_index(level=1, drop=True)\nnew_df = pd.DataFrame(new_df, columns=['Author'])\nnew_df['Author'] = new_df['Author'].str.replace(r'\\(\\d{4}.*\\)', '')\nnew_df['Author'] = new_df['Author'].str.rstrip()","block_group":"cb21fd11b9a843eb82a53f505c0edb04","execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_155/136082988.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n  new_df['Author'] = new_df['Author'].str.replace(r'\\(\\d{4}.*\\)', '')\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"a375588ab8a747c8847b63b984f0bf94","source_hash":"1f724281","execution_start":1684220278541,"execution_millis":149,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"new_df","block_group":"a375588ab8a747c8847b63b984f0bf94","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":1,"row_count":108,"columns":[{"name":"Author","dtype":"object","stats":{"unique_count":95,"nan_count":0,"categories":[{"name":"Gu et al., 2020","count":6},{"name":"Devlin et al., 2019","count":5},{"name":"93 others","count":97}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Author":"Devlin et al., 2019","_deepnote_index_column":"0"},{"Author":" Brown et al., 2020","_deepnote_index_column":"0"},{"Author":"Bommasani\net al., 2021","_deepnote_index_column":"1"},{"Author":"Devlin\net al., 2019","_deepnote_index_column":"2"},{"Author":"Petroni et al.,\n2019","_deepnote_index_column":"3"},{"Author":"Bosselut et al., 2019","_deepnote_index_column":"4"},{"Author":" Raffel et al., 2020","_deepnote_index_column":"4"},{"Author":"Liu et al., 2019","_deepnote_index_column":"5"},{"Author":" Joshi et al., 2020","_deepnote_index_column":"5"},{"Author":"Margolis et al., 1999","_deepnote_index_column":"6"}]},"text/plain":"                                               Author\n0                                 Devlin et al., 2019\n0                                  Brown et al., 2020\n1                             Bommasani\\net al., 2021\n2                                Devlin\\net al., 2019\n3                               Petroni et al.,\\n2019\n..                                                ...\n86  Ansari et al. 2015)Doc B: ... Deep vein thromb...\n87                                Ansari et al. (2015\n88                             Piovella et al.\\n(2002\n89                                 Sheng et al., 2020\n90                                   Jin et al., 2021\n\n[108 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Devlin et al., 2019</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Brown et al., 2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bommasani\\net al., 2021</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Devlin\\net al., 2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Petroni et al.,\\n2019</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>Ansari et al. 2015)Doc B: ... Deep vein thromb...</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>Ansari et al. (2015</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>Piovella et al.\\n(2002</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>Sheng et al., 2020</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>Jin et al., 2021</td>\n    </tr>\n  </tbody>\n</table>\n<p>108 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"a367115ee21d4aaaa22d2a85b6cd4859","deepnote_cell_type":"markdown"},"source":"#### Match les arXiv_ID dans l'article:","block_group":"a367115ee21d4aaaa22d2a85b6cd4859"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a5358386c59e4090854f0a86fd9284ab","source_hash":"90f2d29a","execution_start":1684220278743,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"regex = r\"(arXiv:\\d+\\.\\d+)\"\n\narXiv_doi = pd.DataFrame(columns=['arXiv DOI'])\n\ntest_str = t\n\nmatches = re.finditer(regex, test_str)\n\nfor matchNum, match in enumerate(matches, start=1):\n    print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n    row = [match.group(1)]\n    arXiv_doi.loc[len(arXiv_doi)] = row\n\narXiv_doi['arXiv DOI'] = arXiv_doi['arXiv DOI'].str.replace('arXiv:', '')\n#arXiv_doi\n      ","block_group":"a5358386c59e4090854f0a86fd9284ab","execution_count":6,"outputs":[{"name":"stdout","text":"Match 1 was found at 6038-6054: arXiv:2203.15827\nMatch 2 was found at 52005-52021: arXiv:2107.06955\nMatch 3 was found at 54827-54843: arXiv:2108.07258\nMatch 4 was found at 55969-55985: arXiv:2101.00406\nMatch 5 was found at 58049-58065: arXiv:1704.05179\nMatch 6 was found at 58898-58914: arXiv:2007.15779\nMatch 7 was found at 63164-63180: arXiv:2110.04541\nMatch 8 was found at 64211-64227: arXiv:1907.11692\nMatch 9 was found at 65502-65518: arXiv:2012.14610\nMatch 10 was found at 68383-68399: arXiv:1908.08962\nMatch 11 was found at 69958-69974: arXiv:2201.05966\nMatch 12 was found at 71869-71885: arXiv:1905.07129\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"8fb1273be6ad425eb4812e039a35611e","deepnote_cell_type":"markdown"},"source":"#### Retrouver le nom de l'article et le nom des auteurs avec les DOI en utilisant la bibliotheque arxiv:","block_group":"8fb1273be6ad425eb4812e039a35611e"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"860f85ee6009403c86c967a42d15dad3","source_hash":"3d0b36ee","execution_start":1684220278805,"execution_millis":4373,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Initialiser une DataFrame pour stocker les informations sur les auteurs et les articles\narXiv_info = pd.DataFrame(columns=['DOI','Title', 'Author(s)','Abstract'])\n\n# Boucle à travers les informations stockées dans la DataFrame \"arXiv_doi\"\nfor i, row in arXiv_doi.iterrows():\n    arXiv_id = row['arXiv DOI']\n    #print(arXiv_id)\n    try:\n        # Utiliser la bibliothèque \"arxiv\" pour obtenir les informations sur l'article\n        query = arxiv.Search(arXiv_id)\n        paper = next(query.results())\n\n        title = paper.title\n        \n        authors = ', '.join([author.name for author in paper.authors])\n        #authors = paper.authors[0]\n        abstract = paper.summary\n        \n        #Ajouter les informations à la DataFrame \"arXiv_info\"\n        arXiv_info.loc[i] = [arXiv_id, title, authors,abstract]\n    except:\n        print(f\"Erreur pour retrouver les informations de {arXiv_id}\")\n\n#arXiv_info","block_group":"860f85ee6009403c86c967a42d15dad3","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"f3e6cf61c3c94b6195da47ab28ea8b12","source_hash":"7ad1c492","execution_start":1684220283193,"execution_millis":17,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"arXiv_info","block_group":"f3e6cf61c3c94b6195da47ab28ea8b12","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":12,"columns":[{"name":"DOI","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"2203.15827","count":1},{"name":"2107.06955","count":1},{"name":"10 others","count":10}]}},{"name":"Title","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"LinkBERT: Pretraining Language Models with Document Links","count":1},{"name":"HTLM: Hyper-Text Pre-Training and Prompting of Language Models","count":1},{"name":"10 others","count":10}]}},{"name":"Author(s)","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"Michihiro Yasunaga, Jure Leskovec, Percy Liang","count":1},{"name":"Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, Luke Zettlemoyer","count":1},{"name":"10 others","count":10}]}},{"name":"Abstract","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on\nBioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,\nas well as code and data at https://github.com/michiyasunaga/LinkBERT.","count":1},{"name":"We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.","count":1},{"name":"10 others","count":10}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"DOI":"2203.15827","Title":"LinkBERT: Pretraining Language Models with Document Links","Author(s)":"Michihiro Yasunaga, Jure Leskovec, Percy Liang","Abstract":"Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT s…","_deepnote_index_column":"0"},{"DOI":"2107.06955","Title":"HTLM: Hyper-Text Pre-Training and Prompting of Language Models","Author(s)":"Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, Luke Zettlemoyer","Abstract":"We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of dat…","_deepnote_index_column":"1"},{"DOI":"2108.07258","Title":"On the Opportunities and Risks of Foundation Models","Author(s)":"Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Mun…","Abstract":"AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides pow…","_deepnote_index_column":"2"},{"DOI":"2101.00406","Title":"CDLM: Cross-Document Language Modeling","Author(s)":"Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie Cattan, Ido Dagan","Abstract":"We introduce a new pretraining approach geared for multi-document language\nmodeling, incorporating two key ideas into the masked language modeling\nself-supervised objective. First, instead of considering documents in\nisolation, we pretrain over sets of multiple related documents, encouraging the\nmodel to learn cross-document relationships. Second, we improve over recent\nlong-range transformers by introducing dynamic global attention that has access\nto the entire input to predict masked tokens. We release CDLM (Cross-Document\nLanguage Model), a new general language model for multi-document setting that\ncan be easily applied to downstream tasks. Our extensive analysis shows that\nboth ideas are essential for the success of CDLM, and work in synergy to set\nnew state-of-the-art results for several multi-text tasks. Code and models are\navailable at https://github.com/aviclu/CDLM.","_deepnote_index_column":"3"},{"DOI":"1704.05179","Title":"SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine","Author(s)":"Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, Kyunghyun Cho","Abstract":"We publicly release a new large-scale dataset, called SearchQA, for machine\ncomprehension, or question-answering. Unlike recently released datasets, such\nas DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to\nreflect a full pipeline of general question-answering. That is, we start not\nfrom an existing article and generate a question-answer pair, but start from an\nexisting question-answer pair, crawled from J! Archive, and augment it with\ntext snippets retrieved by Google. Following this approach, we built SearchQA,\nwhich consists of more than 140k question-answer pairs with each pair having\n49.6 snippets on average. Each question-answer-context tuple of the SearchQA\ncomes with additional meta-data such as the snippet's URL, which we believe\nwill be valuable resources for future research. We conduct human evaluation as\nwell as test two baseline methods, one simple word selection and the other deep\nlearning based, on the SearchQA. We show that there is a meaningfu…","_deepnote_index_column":"4"},{"DOI":"2007.15779","Title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing","Author(s)":"Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon","Abstract":"Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining an…","_deepnote_index_column":"5"},{"DOI":"2110.04541","Title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design","Author(s)":"Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua","Abstract":"Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pret…","_deepnote_index_column":"6"},{"DOI":"1907.11692","Title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","Author(s)":"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov","Abstract":"Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.","_deepnote_index_column":"7"},{"DOI":"2012.14610","Title":"UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering","Author(s)":"Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Schlichtkrull, Sonal Gupta, Yashar Mehdad, Scott Yih","Abstract":"We study open-domain question answering with structured, unstructured and\nsemi-structured knowledge sources, including text, tables, lists and knowledge\nbases. Departing from prior work, we propose a unifying approach that\nhomogenizes all sources by reducing them to text and applies the\nretriever-reader model which has so far been limited to text sources only. Our\napproach greatly improves the results on knowledge-base QA tasks by 11 points,\ncompared to latest graph-based methods. More importantly, we demonstrate that\nour unified knowledge (UniK-QA) model is a simple and yet effective way to\ncombine heterogeneous sources of knowledge, advancing the state-of-the-art\nresults on two popular question answering benchmarks, NaturalQuestions and\nWebQuestions, by 3.5 and 2.6 points, respectively.\n  The code of UniK-QA is available at:\nhttps://github.com/facebookresearch/UniK-QA.","_deepnote_index_column":"8"},{"DOI":"1908.08962","Title":"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models","Author(s)":"Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","Abstract":"Recent developments in natural language representations have been accompanied\nby large and expensive models that leverage vast amounts of general-domain text\nthrough self-supervised pre-training. Due to the cost of applying such models\nto down-stream tasks, several model compression techniques on pre-trained\nlanguage representations have been proposed (Sun et al., 2019; Sanh, 2019).\nHowever, surprisingly, the simple baseline of just pre-training and fine-tuning\ncompact models has been overlooked. In this paper, we first show that\npre-training remains important in the context of smaller architectures, and\nfine-tuning pre-trained compact models can be competitive to more elaborate\nmethods proposed in concurrent work. Starting with pre-trained compact models,\nwe then explore transferring task knowledge from large fine-tuned models\nthrough standard knowledge distillation. The resulting simple, yet effective\nand general algorithm, Pre-trained Distillation, brings further improvements.\nThro…","_deepnote_index_column":"9"}]},"text/plain":"           DOI                                              Title  \\\n0   2203.15827  LinkBERT: Pretraining Language Models with Doc...   \n1   2107.06955  HTLM: Hyper-Text Pre-Training and Prompting of...   \n2   2108.07258  On the Opportunities and Risks of Foundation M...   \n3   2101.00406             CDLM: Cross-Document Language Modeling   \n4   1704.05179  SearchQA: A New Q&A Dataset Augmented with Con...   \n5   2007.15779  Domain-Specific Language Model Pretraining for...   \n6   2110.04541  The Inductive Bias of In-Context Learning: Ret...   \n7   1907.11692  RoBERTa: A Robustly Optimized BERT Pretraining...   \n8   2012.14610  UniK-QA: Unified Representations of Structured...   \n9   1908.08962  Well-Read Students Learn Better: On the Import...   \n10  2201.05966  UnifiedSKG: Unifying and Multi-Tasking Structu...   \n11  1905.07129  ERNIE: Enhanced Language Representation with I...   \n\n                                            Author(s)  \\\n0      Michihiro Yasunaga, Jure Leskovec, Percy Liang   \n1   Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, ...   \n2   Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, ...   \n3   Avi Caciularu, Arman Cohan, Iz Beltagy, Matthe...   \n4   Matthew Dunn, Levent Sagun, Mike Higgins, V. U...   \n5   Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, ...   \n6   Yoav Levine, Noam Wies, Daniel Jannai, Dan Nav...   \n7   Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,...   \n8   Barlas Oguz, Xilun Chen, Vladimir Karpukhin, S...   \n9   Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristi...   \n10  Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zh...   \n11  Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang...   \n\n                                             Abstract  \n0   Language model (LM) pretraining can learn vari...  \n1   We introduce HTLM, a hyper-text language model...  \n2   AI is undergoing a paradigm shift with the ris...  \n3   We introduce a new pretraining approach geared...  \n4   We publicly release a new large-scale dataset,...  \n5   Pretraining large neural language models, such...  \n6   Pretraining Neural Language Models (NLMs) over...  \n7   Language model pretraining has led to signific...  \n8   We study open-domain question answering with s...  \n9   Recent developments in natural language repres...  \n10  Structured knowledge grounding (SKG) leverages...  \n11  Neural language representation models such as ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DOI</th>\n      <th>Title</th>\n      <th>Author(s)</th>\n      <th>Abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2203.15827</td>\n      <td>LinkBERT: Pretraining Language Models with Doc...</td>\n      <td>Michihiro Yasunaga, Jure Leskovec, Percy Liang</td>\n      <td>Language model (LM) pretraining can learn vari...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2107.06955</td>\n      <td>HTLM: Hyper-Text Pre-Training and Prompting of...</td>\n      <td>Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, ...</td>\n      <td>We introduce HTLM, a hyper-text language model...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2108.07258</td>\n      <td>On the Opportunities and Risks of Foundation M...</td>\n      <td>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, ...</td>\n      <td>AI is undergoing a paradigm shift with the ris...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2101.00406</td>\n      <td>CDLM: Cross-Document Language Modeling</td>\n      <td>Avi Caciularu, Arman Cohan, Iz Beltagy, Matthe...</td>\n      <td>We introduce a new pretraining approach geared...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1704.05179</td>\n      <td>SearchQA: A New Q&amp;A Dataset Augmented with Con...</td>\n      <td>Matthew Dunn, Levent Sagun, Mike Higgins, V. U...</td>\n      <td>We publicly release a new large-scale dataset,...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2007.15779</td>\n      <td>Domain-Specific Language Model Pretraining for...</td>\n      <td>Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, ...</td>\n      <td>Pretraining large neural language models, such...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2110.04541</td>\n      <td>The Inductive Bias of In-Context Learning: Ret...</td>\n      <td>Yoav Levine, Noam Wies, Daniel Jannai, Dan Nav...</td>\n      <td>Pretraining Neural Language Models (NLMs) over...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1907.11692</td>\n      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n      <td>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,...</td>\n      <td>Language model pretraining has led to signific...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2012.14610</td>\n      <td>UniK-QA: Unified Representations of Structured...</td>\n      <td>Barlas Oguz, Xilun Chen, Vladimir Karpukhin, S...</td>\n      <td>We study open-domain question answering with s...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1908.08962</td>\n      <td>Well-Read Students Learn Better: On the Import...</td>\n      <td>Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristi...</td>\n      <td>Recent developments in natural language repres...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2201.05966</td>\n      <td>UnifiedSKG: Unifying and Multi-Tasking Structu...</td>\n      <td>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zh...</td>\n      <td>Structured knowledge grounding (SKG) leverages...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1905.07129</td>\n      <td>ERNIE: Enhanced Language Representation with I...</td>\n      <td>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang...</td>\n      <td>Neural language representation models such as ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0c857bb3845547d9adbecba73594d380","deepnote_cell_type":"markdown"},"source":"#### Extraction du contexte de citation avec le nom du premier auteur dans le texte:","block_group":"0c857bb3845547d9adbecba73594d380"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"086606e5fe844e4ea513318f76a29d76","source_hash":"c715ef43","execution_start":1684220283263,"execution_millis":322,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df_author_context = pd.DataFrame(columns=['Author','context_left','context_right'])\n\nfor index, row in new_df.iterrows():\n    authors = row['Author']\n    first_author = authors.split(',')[0]\n    author_index = t.find(first_author)\n    if author_index != -1:\n        author_start = max(0, author_index - 30 * len(first_author))\n        author_end = min(len(t), author_index + 30 * len(first_author))\n        #context = t[author_start:author_end]\n        #print(\"Le nom du premier auteur '{}' a été trouvé dans le texte pour l'article '{}' avec le DOI '{}' avec le contexte suivant :\\n{}\".format(first_author, title, doi, context))\n        context_left = t[author_start:author_index]\n        context_right = t[author_index:author_end]\n        df_author_context = df_author_context.append({'Author': first_author, 'context_left': context_left, 'context_right': context_right}, ignore_index=True)\n\n#df_author_context\n","block_group":"086606e5fe844e4ea513318f76a29d76","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d41f621a72b14f7abd305e97cf349f4a","source_hash":"5f3c60fc","execution_start":1684220283599,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"arXiv_info['First_Author_Last_Name'] = arXiv_info['Author(s)'].apply(lambda x: x.split(',')[0].strip().split(' ')[-1])\n\n#merged_df = pd.merge(df_author_context, arXiv_info, left_on='Author', right_on='Author(s)')\n#merged_df\narXiv_info['First_Author_Last_Name']","block_group":"d41f621a72b14f7abd305e97cf349f4a","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"0       Yasunaga\n1     Aghajanyan\n2      Bommasani\n3      Caciularu\n4           Dunn\n5             Gu\n6         Levine\n7            Liu\n8           Oguz\n9           Turc\n10           Xie\n11         Zhang\nName: First_Author_Last_Name, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b553efdc128a4c8bb6bbab04c251a275","deepnote_cell_type":"markdown"},"source":"#### Formatage du dataframe df_author_context","block_group":"b553efdc128a4c8bb6bbab04c251a275"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1d630a3091a548d9b16775b963303834","source_hash":"5743b2d5","execution_start":1684220283636,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df_author_context['Author']=df_author_context['Author'].apply(lambda x: x.split()[0])\n\ndf_author_context_arXiv_only = df_author_context[df_author_context['Author'].isin(arXiv_info['First_Author_Last_Name'].values)]\ndf_author_context_arXiv_only = df_author_context_arXiv_only.merge(arXiv_info[['First_Author_Last_Name', 'DOI', 'Title','Abstract']],\n                                                                  left_on='Author', right_on='First_Author_Last_Name',\n                                                                  how='left')\n\ndf_author_context_arXiv_only.drop(df_author_context_arXiv_only.columns[[0,3,5]], axis=1,inplace=True)\n\n\ndf_author_context_arXiv_only= df_author_context_arXiv_only.drop_duplicates(subset='DOI', keep='first')","block_group":"1d630a3091a548d9b16775b963303834","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0e31278de12c45c8b77d0281703f7f0c","source_hash":"3b926f11","execution_start":1684220283664,"execution_millis":2,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df_author_context_arXiv_only","block_group":"0e31278de12c45c8b77d0281703f7f0c","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":12,"columns":[{"name":"context_left","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"ur biomedical\nLinkBERT sets new states of the art on various\nBioNLP tasks (+7% on BioASQ and USMLE).\nWe release our pretrained models, LinkBERT\nandBioLinkBERT , as well as code and data.1\n1 Introduction\nPretrained language models (LMs), like BERT and\nGPTs (Devlin et al., 2019; Brown et al., 2020), have\nshown remarkable performance on many natural\nlanguage processing (NLP) tasks, such as text\nclassiﬁcation and question answering, becoming the\nfoundation of modern NLP systems (","count":1},{"name":"us applications, including\nanswering a question “What trees can you see at Tidal Basin?”.\nWe aim to leverage document links to incorporate more\nknowledge into language model pretraining.\nHowever, existing LM pretraining methods typ-\nically consider text from a single document in each\ninput context (","count":1},{"name":"10 others","count":10}]}},{"name":"context_right","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"Bommasani\net al., 2021). By performing self-supervised learn-\ning, such as masked language modeling (Devlin\net al., 2019), LMs learn to encode various knowl-\nedge from text corpora and produce informative\nrepresentations for downstream tasks (Petroni et al.,\n2019; Bosselut et al., 2019; Raffel et al., 2020).\n\u0003Equal senior authorship.\n1Available at https://github.com/michiyasunaga/\nLinkBERT .\nLanguage Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment AS","count":1},{"name":"Liu et al., 2019; Joshi et al., 2020)\nand do not model links between documents. This\ncan pose limitations because documents often have\nrich dependencies (e.g. hyperlinks, references), and\nknowledge can span across documents. As an exam-\nple, in Figure 1, the Wikipedia article “Tidal Basin,\nWashingto","count":1},{"name":"10 others","count":10}]}},{"name":"DOI","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"2108.07258","count":1},{"name":"1907.11692","count":1},{"name":"10 others","count":10}]}},{"name":"Abstract","dtype":"object","stats":{"unique_count":12,"nan_count":0,"categories":[{"name":"AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides powerful leverage but demands caution,\nas the defects of the foundation model are inherited by all the adapted models\ndownstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and\nwhat they are even capable of due to their emergent properties. To tackle these\nquestions, we believe much of the critical research on foundation models will\nrequire deep interdisciplinary collaboration commensurate with their\nfundamentally sociotechnical nature.","count":1},{"name":"Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.","count":1},{"name":"10 others","count":10}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"context_left":"ur biomedical\nLinkBERT sets new states of the art on various\nBioNLP tasks (+7% on BioASQ and USMLE).\nWe release our pretrained models, LinkBERT\nandBioLinkBERT , as well as code and data.1\n1 Introduction\nPretrained language models (LMs), like BERT and\nGPTs (Devlin et al., 2019; Brown et al., 2020), have\nshown remarkable performance on many natural\nlanguage processing (NLP) tasks, such as text\nclassiﬁcation and question answering, becoming the\nfoundation of modern NLP systems (","context_right":"Bommasani\net al., 2021). By performing self-supervised learn-\ning, such as masked language modeling (Devlin\net al., 2019), LMs learn to encode various knowl-\nedge from text corpora and produce informative\nrepresentations for downstream tasks (Petroni et al.,\n2019; Bosselut et al., 2019; Raffel et al., 2020).\n\u0003Equal senior authorship.\n1Available at https://github.com/michiyasunaga/\nLinkBERT .\nLanguage Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment AS","DOI":"2108.07258","Abstract":"AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides pow…","_deepnote_index_column":"0"},{"context_left":"us applications, including\nanswering a question “What trees can you see at Tidal Basin?”.\nWe aim to leverage document links to incorporate more\nknowledge into language model pretraining.\nHowever, existing LM pretraining methods typ-\nically consider text from a single document in each\ninput context (","context_right":"Liu et al., 2019; Joshi et al., 2020)\nand do not model links between documents. This\ncan pose limitations because documents often have\nrich dependencies (e.g. hyperlinks, references), and\nknowledge can span across documents. As an exam-\nple, in Figure 1, the Wikipedia article “Tidal Basin,\nWashingto","DOI":"1907.11692","Abstract":"Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.","_deepnote_index_column":"1"},{"context_left":" in both domains.LinkBERT consistently improves on baseline LMs\nacross domains and tasks. For the general domain,\nLinkBERT outperforms BERT on MRQA bench-\nmark (+4% absolute in F1-score) as well as GLUE\nbenchmark. For the biomedical domain, LinkBERT\nexceeds PubmedBERT (","context_right":"Gu et al., 2020) and sets\nnew states of the art on BLURB biomedical NLP\nbenchmark (+3% absolute in BLURB score) and\nMedQA-USMLE reasoning task (+7% absolute in\naccuracy). Overall, LinkBERT attains notably large\ngains for multi-hop reasoning, multi-document\nunderstanding","DOI":"2007.15779","Abstract":"Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining an…","_deepnote_index_column":"2"},{"context_left":"arge\ngains for multi-hop reasoning, multi-document\nunderstanding, and few-shot question answering,\nsuggesting that LinkBERT internalizes signiﬁcantly\nmore knowledge than existing LMs by pretraining\nwith document link information.\n2 Related work\nRetrieval-augmented LMs. Several works\n(Lewis et al., 2020b; Karpukhin et al., 2020; ","context_right":"Oguz\net al., 2020; Xie et al., 2022) introduce a retrieval\nmodule for LMs, where given an anchor text\n(e.g. question), retrieved text is added to the same\nLM context to improve model inference (e.g. an-\nswer prediction). These works show the promise of\nplacing related documents in the same LM context\nat inference time, but they ","DOI":"2012.14610","Abstract":"We study open-domain question answering with structured, unstructured and\nsemi-structured knowledge sources, including text, tables, lists and knowledge\nbases. Departing from prior work, we propose a unifying approach that\nhomogenizes all sources by reducing them to text and applies the\nretriever-reader model which has so far been limited to text sources only. Our\napproach greatly improves the results on knowledge-base QA tasks by 11 points,\ncompared to latest graph-based methods. More importantly, we demonstrate that\nour unified knowledge (UniK-QA) model is a simple and yet effective way to\ncombine heterogeneous sources of knowledge, advancing the state-of-the-art\nresults on two popular question answering benchmarks, NaturalQuestions and\nWebQuestions, by 3.5 and 2.6 points, respectively.\n  The code of UniK-QA is available at:\nhttps://github.com/facebookresearch/UniK-QA.","_deepnote_index_column":"3"},{"context_left":"ti-hop reasoning, multi-document\nunderstanding, and few-shot question answering,\nsuggesting that LinkBERT internalizes signiﬁcantly\nmore knowledge than existing LMs by pretraining\nwith document link information.\n2 Related work\nRetrieval-augmented LMs. Several works\n(Lewis et al., 2020b; Karpukhin et al., 2020; Oguz\net al., 2020;","context_right":" Xie et al., 2022) introduce a retrieval\nmodule for LMs, where given an anchor text\n(e.g. question), retrieved text is added to the same\nLM context to improve model inference (e.g. an-\nswer prediction). These works show the promise of\nplacing related documents in the same LM context\nat inference time, but they do not study the e","DOI":"2201.05966","Abstract":"Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learn…","_deepnote_index_column":"4"},{"context_left":" LM with a retriever that learns to retrieve text for\nanswering masked tokens in the anchor text. In con-\ntrast, our focus is not on retrieval, but on pretraining\na general-purpose LM that internalizes knowledge\nthat spans across documents, which is orthogonal\nto the above works (e.g., our pretrained LM could\nbe used to initialize the LM component of these\nworks). Additionally, we focus on incorporating\ndocument links such as hyperlinks, which can offer\nsalient knowledge that common lexical retrieval\nmethods may not provide (Asai et al., 2020).\nPretrain LMs with related documents. Several\nconcurrent works use multiple related documentsto pretrain LMs. ","context_right":"Caciularu et al. (2021) place doc-\numents (news articles) about the same topic into the\nsame LM context, and Levine et al. (2021) place sen-\ntences of high lexical similarity into the same con-\ntext. Our work provides a general method to incor-\nporate document links into LM pretraining, where\nlexical or topical similarity can be one instance of\ndocument links, besides hyperlinks. We focus on hy-\nperlinks in this work, because we ﬁnd they can bring\nin salient knowledge that may not be obvious via\nlexical similarity, and yield a more performant LM\n(§5.5). Additionally, we propose the DRP objective,\nwhich improves modeling multiple documents and\nrelations","DOI":"2101.00406","Abstract":"We introduce a new pretraining approach geared for multi-document language\nmodeling, incorporating two key ideas into the masked language modeling\nself-supervised objective. First, instead of considering documents in\nisolation, we pretrain over sets of multiple related documents, encouraging the\nmodel to learn cross-document relationships. Second, we improve over recent\nlong-range transformers by introducing dynamic global attention that has access\nto the entire input to predict masked tokens. We release CDLM (Cross-Document\nLanguage Model), a new general language model for multi-document setting that\ncan be easily applied to downstream tasks. Our extensive analysis shows that\nboth ideas are essential for the success of CDLM, and work in synergy to set\nnew state-of-the-art results for several multi-text tasks. Code and models are\navailable at https://github.com/aviclu/CDLM.","_deepnote_index_column":"5"},{"context_left":"es knowledge\nthat spans across documents, which is orthogonal\nto the above works (e.g., our pretrained LM could\nbe used to initialize the LM component of these\nworks). Additionally, we focus on incorporating\ndocument links such as hyperlinks, which can offer\nsalient knowledge that common lexical retrieval\nmethods may not provide (Asai et al., 2020).\nPretrain LMs with related documents. Several\nconcurrent works use multiple related documentsto pretrain LMs. Caciularu et al. (2021) place doc-\numents (news articles) about the same topic into the\nsame LM context, and ","context_right":"Levine et al. (2021) place sen-\ntences of high lexical similarity into the same con-\ntext. Our work provides a general method to incor-\nporate document links into LM pretraining, where\nlexical or topical similarity can be one instance of\ndocument links, besides hyperlinks. We focus on hy-\nperlinks in this work, because we ﬁnd they can bring\nin salient knowledge that may not be obvious via\nlexical similarity, and yield a more performant LM\n(§5.5). Additionally, we propose the DRP objective,\nwhich improves modeling multiple documents and\nrelations between them in LM","DOI":"2110.04541","Abstract":"Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pret…","_deepnote_index_column":"6"},{"context_left":"uestion answering.\nMa et al. (2021) study various hyperlink-aware\npretraining tasks for retrieval. While these works\nuse hyperlinks to learn retrievers, we focus on using\nhyperlinks to create better context for learning\ngeneral-purpose LMs. Separately, Calixto et al.\n(2021) use Wikipedia hyperlinks to learn multilin-\ngual LMs. Citation links are often used to improve\nsummarization and recommendation of academic\npapers (Qazvinian and Radev, 2008; ","context_right":"Yasunaga et al.,\n2019; Bhagavatula et al., 2018; Khadka et al., 2020;\nCohan et al., 2020). Here we leverage citation net-\nworks to improve pretraining general-purpose LMs.\nGraph-augmented LMs. Several works aug-\nment LMs with graphs, typically, knowledge graphs\n(KGs) where the nodes capture entities and edges\ntheir relations. Zhang et al. (2019); He et al. (2020);\nWang et al. (2021b) combine LM training with\nKG embeddings. Sun et al. (2020); Yasu","DOI":"2203.15827","Abstract":"Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT s…","_deepnote_index_column":"7"},{"context_left":"ers (Qazvinian and Radev, 2008; Yasunaga et al.,\n2019; Bhagavatula et al., 2018; Khadka et al., 2020;\nCohan et al., 2020). Here we leverage citation net-\nworks to improve pretraining general-purpose LMs.\nGraph-augmented LMs. Several works aug-\nment LMs with graphs, typically, knowledge graphs\n(KGs) where the nodes capture entities and edges\ntheir relations. ","context_right":"Zhang et al. (2019); He et al. (2020);\nWang et al. (2021b) combine LM training with\nKG embeddings. Sun et al. (2020); Yasunaga et al.\n(2021); Zhang et al. (2022) combine LMs and graph\nneural networks (GNNs) to jointly train on text and\nKGs. Different from KGs, we use document graphs\nto learn knowledge that spans across documents.\n3 Preliminaries\nA language m","DOI":"1905.07129","Abstract":"Neural language representation models such as BERT pre-trained on large-scale\ncorpora can well capture rich semantic patterns from plain text, and be\nfine-tuned to consistently improve the performance of various NLP tasks.\nHowever, the existing pre-trained language models rarely consider incorporating\nknowledge graphs (KGs), which can provide rich structured knowledge facts for\nbetter language understanding. We argue that informative entities in KGs can\nenhance language representation with external knowledge. In this paper, we\nutilize both large-scale textual corpora and KGs to train an enhanced language\nrepresentation model (ERNIE), which can take full advantage of lexical,\nsyntactic, and knowledge information simultaneously. The experimental results\nhave demonstrated that ERNIE achieves significant improvements on various\nknowledge-driven tasks, and meanwhile is comparable with the state-of-the-art\nmodel BERT on other common NLP tasks. The source code of this paper can be\nobtained f…","_deepnote_index_column":"8"},{"context_left":"§6). Hy-\nperlinks have a number of advantages. They provide\nbackground knowledge about concepts that the doc-\nument writers deemed useful—the links are likely\nto have high precision of relevance, and can also\nbring in relevant documents that may not be obvious\nvia lexical similarity alone (e.g., in Figure 1, while\nthe hyperlinked article mentions “Japanese” and\n“Yoshino” cherry trees, these words do not appear in\nthe anchor article). Hyperlinks are also ubiquitous\non the web and easily gathered at scale (","context_right":"Aghajanyan\net al., 2021). To construct the document graph, we\nsimply make a directed edge (X(i);X(j))if there is\na hyperlink from document X(i)to document X(j).\nFor comparison, we also experiment with a docu-\nment graph built by lexical similarity between docu-\nments. For each document X(i), we use the common\nTF-IDF cosine similarity metric (Chen et al., 2017;\nYasunaga et al., 2017) to obtain top- kdocuments\nX(j)’s and make edges (X(i);X(j)). We usek=5.\n4.2 Pretraining tasks\nCreating input instances. Seve","DOI":"2107.06955","Abstract":"We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of dat…","_deepnote_index_column":"11"}]},"text/plain":"                                         context_left  \\\n0   ur biomedical\\nLinkBERT sets new states of the...   \n1   us applications, including\\nanswering a questi...   \n2    in both domains.LinkBERT consistently improve...   \n3   arge\\ngains for multi-hop reasoning, multi-doc...   \n4   ti-hop reasoning, multi-document\\nunderstandin...   \n5    LM with a retriever that learns to retrieve t...   \n6   es knowledge\\nthat spans across documents, whi...   \n7   uestion answering.\\nMa et al. (2021) study var...   \n8   ers (Qazvinian and Radev, 2008; Yasunaga et al...   \n11  §6). Hy-\\nperlinks have a number of advantages...   \n15   and BookCorpus to train\\nLinkBERT. In summary...   \n16  answering (QA). Given a\\ndocument (or set of d...   \n\n                                        context_right         DOI  \\\n0   Bommasani\\net al., 2021). By performing self-s...  2108.07258   \n1   Liu et al., 2019; Joshi et al., 2020)\\nand do ...  1907.11692   \n2   Gu et al., 2020) and sets\\nnew states of the a...  2007.15779   \n3   Oguz\\net al., 2020; Xie et al., 2022) introduc...  2012.14610   \n4    Xie et al., 2022) introduce a retrieval\\nmodu...  2201.05966   \n5   Caciularu et al. (2021) place doc-\\numents (ne...  2101.00406   \n6   Levine et al. (2021) place sen-\\ntences of hig...  2110.04541   \n7   Yasunaga et al.,\\n2019; Bhagavatula et al., 20...  2203.15827   \n8   Zhang et al. (2019); He et al. (2020);\\nWang e...  1905.07129   \n11  Aghajanyan\\net al., 2021). To construct the do...  2107.06955   \n15   Turc et al., 2019). We\\nuse -tiny mainly for ...  1908.08962   \n16  Dunn et al., 2017), NewsQA (Trischler\\net al.,...  1704.05179   \n\n                                             Abstract  \n0   AI is undergoing a paradigm shift with the ris...  \n1   Language model pretraining has led to signific...  \n2   Pretraining large neural language models, such...  \n3   We study open-domain question answering with s...  \n4   Structured knowledge grounding (SKG) leverages...  \n5   We introduce a new pretraining approach geared...  \n6   Pretraining Neural Language Models (NLMs) over...  \n7   Language model (LM) pretraining can learn vari...  \n8   Neural language representation models such as ...  \n11  We introduce HTLM, a hyper-text language model...  \n15  Recent developments in natural language repres...  \n16  We publicly release a new large-scale dataset,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context_left</th>\n      <th>context_right</th>\n      <th>DOI</th>\n      <th>Abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ur biomedical\\nLinkBERT sets new states of the...</td>\n      <td>Bommasani\\net al., 2021). By performing self-s...</td>\n      <td>2108.07258</td>\n      <td>AI is undergoing a paradigm shift with the ris...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>us applications, including\\nanswering a questi...</td>\n      <td>Liu et al., 2019; Joshi et al., 2020)\\nand do ...</td>\n      <td>1907.11692</td>\n      <td>Language model pretraining has led to signific...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>in both domains.LinkBERT consistently improve...</td>\n      <td>Gu et al., 2020) and sets\\nnew states of the a...</td>\n      <td>2007.15779</td>\n      <td>Pretraining large neural language models, such...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>arge\\ngains for multi-hop reasoning, multi-doc...</td>\n      <td>Oguz\\net al., 2020; Xie et al., 2022) introduc...</td>\n      <td>2012.14610</td>\n      <td>We study open-domain question answering with s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ti-hop reasoning, multi-document\\nunderstandin...</td>\n      <td>Xie et al., 2022) introduce a retrieval\\nmodu...</td>\n      <td>2201.05966</td>\n      <td>Structured knowledge grounding (SKG) leverages...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LM with a retriever that learns to retrieve t...</td>\n      <td>Caciularu et al. (2021) place doc-\\numents (ne...</td>\n      <td>2101.00406</td>\n      <td>We introduce a new pretraining approach geared...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>es knowledge\\nthat spans across documents, whi...</td>\n      <td>Levine et al. (2021) place sen-\\ntences of hig...</td>\n      <td>2110.04541</td>\n      <td>Pretraining Neural Language Models (NLMs) over...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>uestion answering.\\nMa et al. (2021) study var...</td>\n      <td>Yasunaga et al.,\\n2019; Bhagavatula et al., 20...</td>\n      <td>2203.15827</td>\n      <td>Language model (LM) pretraining can learn vari...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ers (Qazvinian and Radev, 2008; Yasunaga et al...</td>\n      <td>Zhang et al. (2019); He et al. (2020);\\nWang e...</td>\n      <td>1905.07129</td>\n      <td>Neural language representation models such as ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>§6). Hy-\\nperlinks have a number of advantages...</td>\n      <td>Aghajanyan\\net al., 2021). To construct the do...</td>\n      <td>2107.06955</td>\n      <td>We introduce HTLM, a hyper-text language model...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>and BookCorpus to train\\nLinkBERT. In summary...</td>\n      <td>Turc et al., 2019). We\\nuse -tiny mainly for ...</td>\n      <td>1908.08962</td>\n      <td>Recent developments in natural language repres...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>answering (QA). Given a\\ndocument (or set of d...</td>\n      <td>Dunn et al., 2017), NewsQA (Trischler\\net al.,...</td>\n      <td>1704.05179</td>\n      <td>We publicly release a new large-scale dataset,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"f38998550b0f4bbe86af28d4a6a29e77","deepnote_cell_type":"markdown"},"source":"#### Supprimer les arxiv_id en double","block_group":"f38998550b0f4bbe86af28d4a6a29e77"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"afa625c67c3545b4afb901ea31caf555","source_hash":"f2ced06f","execution_start":1684220283721,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df_author_context_arXiv_only= df_author_context_arXiv_only.drop_duplicates(subset='DOI', keep='first')","block_group":"afa625c67c3545b4afb901ea31caf555","execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"634fa606761d4734a1f3064e7288817f","deepnote_cell_type":"markdown"},"source":"#### Parse en CSV le dataframe final","block_group":"634fa606761d4734a1f3064e7288817f"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e8837a8eb578430d8deae17bcdd2e923","source_hash":"4c370dba","execution_start":1684220283723,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"df_author_context_arXiv_only.to_csv(\"df_author_context_arXiv_only.csv\", index=False)\n","block_group":"e8837a8eb578430d8deae17bcdd2e923","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"873d0069f2ba47a5981921c1413a8ad4","deepnote_cell_type":"markdown"},"source":"## fonction qui genere un datastet de contexte pour un article ","block_group":"873d0069f2ba47a5981921c1413a8ad4"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"16ae73fc4ffe4d66b773077722b5cc4a","source_hash":"a5216ab0","execution_start":1684220283724,"execution_millis":34,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import PyPDF2\nimport re\nimport pandas as pd\nimport arxiv\n\ndef get_df_author_context_arXiv_only(file_path):\n    with open(file_path, 'rb') as file:\n        # créer un objet pdf\n        pdf = PyPDF2.PdfReader(file)\n         #extraire le texte de chaque page\n        t=''\n        for page in range (len(pdf.pages)): \n            t+=pdf.pages[page].extract_text()\n            #print(pdf.pages[page].extract_text())\n        print(t) #t stocke notre texte extrait du PDF\n\n\n    new_df = pd.DataFrame(columns=['Author'])\n\n    regex = r\"((?:[A-Z][A-Za-z'`-]+\\s*et al.*\\s*.\\d{4}))|((?:[A-Z][a-z]*\\s+)(?:[a-z]*\\s*)(?:[A-Z][a-z]*)(?:,\\s*)(?:\\d{4}))|(?:[A-Z][a-z]*\\s+)(?:et al.?)(?:,\\s)(?:\\d{4})|((?:[A-Z][a-z]*\\s+)(?:al.?)(?:,\\s*)(?:\\d{4}))\"\n\n    test_str = t\n\n    matches = re.finditer(regex, test_str, re.MULTILINE)\n\n    for matchNum, match in enumerate(matches, start=1):\n\n        row = [match.group()]\n        new_df.loc[len(new_df)] = row\n\n\n    new_df = new_df['Author'].str.split(';', expand=True).stack().reset_index(level=1, drop=True)\n    new_df = pd.DataFrame(new_df, columns=['Author'])\n    new_df['Author'] = new_df['Author'].str.replace(r'\\(\\d{4}.*\\)', '')\n    new_df['Author'] = new_df['Author'].str.rstrip()\n\n\n    regex = r\"(arXiv:\\d+\\.\\d+)\"\n\n    arXiv_doi = pd.DataFrame(columns=['arXiv DOI'])\n\n    test_str = t\n\n    matches = re.finditer(regex, test_str)\n\n    for matchNum, match in enumerate(matches, start=1):\n        print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n        row = [match.group(1)]\n        arXiv_doi.loc[len(arXiv_doi)] = row\n\n    arXiv_doi['arXiv DOI'] = arXiv_doi['arXiv DOI'].str.replace('arXiv:', '')\n\n    # Initialiser une DataFrame pour stocker les informations sur les auteurs et les articles\n    arXiv_info = pd.DataFrame(columns=['DOI','Title', 'Author(s)','Abstract'])\n\n    # Boucle à travers les informations stockées dans la DataFrame \"arXiv_doi\"\n    for i, row in arXiv_doi.iterrows():\n        arXiv_id = row['arXiv DOI']\n        #print(arXiv_id)\n        try:\n            # Utiliser la bibliothèque \"arxiv\" pour obtenir les informations sur l'article\n            query = arxiv.Search(arXiv_id)\n            paper = next(query.results())\n\n            title = paper.title\n\n            authors = ', '.join([author.name for author in paper.authors])\n            #authors = paper.authors[0]\n            abstract = paper.summary\n\n            #Ajouter les informations\n            arXiv_info = arXiv_info.append({'DOI': arXiv_id,\n                                        'Title': title,\n                                        'Author(s)': authors,\n                                        'Abstract': abstract},\n                                        ignore_index=True)\n        except Exception as e:\n            print(f\"Error: {e} - Skipping arXiv ID: {arXiv_id}\")\n    df_author_context = pd.DataFrame(columns=['Author','context_left','context_right'])\n\n    for index, row in new_df.iterrows():\n        authors = row['Author']\n    first_author = authors.split(',')[0]\n    author_index = t.find(first_author)\n    if author_index != -1:\n        author_start = max(0, author_index - 30 * len(first_author))\n        author_end = min(len(t), author_index + 30 * len(first_author))\n        #context = t[author_start:author_end]\n        #print(\"Le nom du premier auteur '{}' a été trouvé dans le texte pour l'article '{}' avec le DOI '{}' avec le contexte suivant :\\n{}\".format(first_author, title, doi, context))\n        context_left = t[author_start:author_index]\n        context_right = t[author_index:author_end]\n        df_author_context = df_author_context.append({'Author': first_author, 'context_left': context_left, 'context_right': context_right}, ignore_index=True)\n\n    #df_author_context\n    arXiv_info['First_Author_Last_Name'] = arXiv_info['Author(s)'].apply(lambda x: x.split(',')[0].strip().split(' ')[-1])\n    df_author_context['Author']=df_author_context['Author'].apply(lambda x: x.split()[0])\n\n    df_author_context_arXiv_only = df_author_context[df_author_context['Author'].isin(arXiv_info['First_Author_Last_Name'].values)]\n    df_author_context_arXiv_only = df_author_context_arXiv_only.merge(arXiv_info[['First_Author_Last_Name', 'DOI', 'Title','Abstract']],\n                                                                  left_on='Author', right_on='First_Author_Last_Name',\n                                                                  how='left')\n\n    df_author_context_arXiv_only.drop(df_author_context_arXiv_only.columns[[0,3,5]], axis=1,inplace=True)\n\n\n    df_author_context_arXiv_only= df_author_context_arXiv_only.drop_duplicates(subset='DOI', keep='first')\n\n    return df_author_context_arXiv_only","block_group":"16ae73fc4ffe4d66b773077722b5cc4a","execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"badc40372cb14c11aa2eac1c4e62c75a","deepnote_cell_type":"markdown"},"source":"## Fonction pour générer un graphe de citation sur un article ","block_group":"badc40372cb14c11aa2eac1c4e62c75a"},{"cell_type":"code","metadata":{"source_hash":"73aac2ea","execution_start":1684220283772,"execution_millis":5225,"deepnote_to_be_reexecuted":false,"cell_id":"ca26ef7d0eeb4ed3a96e9cf0d098c18d","deepnote_cell_type":"code"},"source":"!pip install networkx==3.0\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport PyPDF2\nimport re\nimport pandas as pd\nimport arxiv\n\nimport plotly.graph_objects as go\n\n\n# Extraction de texte à partir d'un PDF\ndef extract_text_from_pdf(file_path):\n    with open(file_path, 'rb') as file:\n        pdf = PyPDF2.PdfReader(file)\n        text = ''\n        for page in pdf.pages:\n            text += page.extract_text()\n        return text\n\n# Extraction des noms d'auteurs et des identifiants arXiv\ndef extract_authors_and_arxiv_ids(text):\n    # Regex pour extraire les noms d'auteurs\n    author_regex = r\"((?:[A-Z][A-Za-z'`-]+\\s*et al.*\\s*.\\d{4}))|((?:[A-Z][a-z]*\\s+)(?:[a-z]*\\s*)(?:[A-Z][a-z]*)(?:,\\s*)(?:\\d{4}))|(?:[A-Z][a-z]*\\s+)(?:et al.?)(?:,\\s)(?:\\d{4})|((?:[A-Z][a-z]*\\s+)(?:al.?)(?:,\\s*)(?:\\d{4}))\"\n    # Regex pour extraire les identifiants arXiv\n    arxiv_regex = r\"(arXiv:\\d+\\.\\d+)\"\n\n    # Extraction des noms d'auteurs et des identifiants arXiv\n    authors = []\n    arxiv_ids = []\n    for match in re.finditer(author_regex, text, re.MULTILINE):\n        authors.append(match.group().rstrip())\n    for match in re.finditer(arxiv_regex, text):\n        arxiv_ids.append(match.group(1).replace('arXiv:', ''))\n\n    return authors, arxiv_ids\n\n# Récupération des informations des articles à partir des identifiants arXiv\ndef get_arxiv_info(arxiv_ids):\n    arxiv_info = pd.DataFrame(columns=['DOI', 'Title', 'Author(s)', 'Abstract'])\n    for i, arxiv_id in enumerate(arxiv_ids):\n        try:\n            paper = next(arxiv.Search(arxiv_id).results())\n            arxiv_info.loc[i] = [arxiv_id, paper.title, ', '.join([author.name for author in paper.authors]), paper.summary]\n        except:\n            print(f\"Erreur pour retrouver les informations de {arxiv_id}\")\n    return arxiv_info\n\n# Création et visualisation du graphe des citations\n\n\ndef create_citation_graph(arxiv_info):\n    G = nx.DiGraph()\n    central_node = arxiv_info.iloc[0]['DOI']\n    G.add_node(central_node)\n    for i, row in arxiv_info.iloc[1:].iterrows():\n        doi = row['DOI']\n        G.add_node(doi)\n        G.add_edge(central_node, doi)\n\n    # Création de la liste de noms d'auteurs pour chaque nœud\n    node_labels = {}\n    for i, row in arxiv_info.iterrows():\n        doi = row['DOI']\n        node_labels[doi] = '<b>' + row['Title'] + '</b><br>' + row['Author(s)'] + '<br>' + doi\n\n    # Définition des positions des nœuds à l'aide de l'algorithme de Fruchterman-Reingold\n    pos = nx.fruchterman_reingold_layout(G)\n\n    # Création du graphique avec Plotly\n    edge_x = []\n    edge_y = []\n    for edge in G.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x.append(x0)\n        edge_x.append(x1)\n        edge_x.append(None)\n        edge_y.append(y0)\n        edge_y.append(y1)\n        edge_y.append(None)\n\n    node_x = []\n    node_y = []\n    node_text = []\n    for node in G.nodes():\n        x, y = pos[node]\n        node_x.append(x)\n        node_y.append(y)\n        node_text.append(node_labels[node])\n\n    fig = go.Figure()\n\n    # Ajout des arêtes et des nœuds\n    fig.add_trace(go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='gray'), hoverinfo='none', mode='lines'))\n    fig.add_trace(go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text', marker=dict(symbol='circle', size=10, color='lightblue'), text=node_text))\n\n    # Mise en forme du graphique\n    fig.update_layout(title='Graphique des citations', title_x=0.5, title_font_size=20, width=800, height=600, showlegend=False, hovermode='closest')\n\n    fig.show()\n\n\n# Chemin complet : extraction de texte, extraction des noms d'auteurs et des identifiants arXiv,\n# récupération des informations des articles à partir des identifiants arXiv, création et visualisation du graphe des citations\ndef process_citation_graph(file_path):\n    text = extract_text_from_pdf(file_path)\n    authors, arxiv_ids = extract_authors_and_arxiv_ids(text)\n    arxiv_info = get_arxiv_info(arxiv_ids)\n    create_citation_graph(arxiv_info)\n\n","block_group":"ca26ef7d0eeb4ed3a96e9cf0d098c18d","execution_count":16,"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx==3.0 in /root/venv/lib/python3.9/site-packages (3.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"16c8b6fcda484fb78b89658aa234b257","source_hash":"6b1605c1","execution_start":1684220289000,"execution_millis":5606,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"process_citation_graph('/work/articletest.pdf')","block_group":"16c8b6fcda484fb78b89658aa234b257","execution_count":17,"outputs":[{"data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"3bcc8a50-18de-4738-ae23-8f81c66f57f4\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3bcc8a50-18de-4738-ae23-8f81c66f57f4\")) {                    Plotly.newPlot(                        \"3bcc8a50-18de-4738-ae23-8f81c66f57f4\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"gray\",\"width\":0.5},\"mode\":\"lines\",\"x\":[0.0056691370406218005,0.34704563667013444,null,0.0056691370406218005,-0.6615856134080516,null,0.0056691370406218005,0.78552175504422,null,0.0056691370406218005,-0.8600272204102756,null,0.0056691370406218005,0.7364944133101362,null,0.0056691370406218005,0.5584055319426203,null,0.0056691370406218005,-0.6311101411542009,null,0.0056691370406218005,-0.17241632276139432,null,0.0056691370406218005,0.9999999999999999,null,0.0056691370406218005,-0.20511849744920788,null,0.0056691370406218005,-0.9028786788246024,null],\"y\":[0.00531037562885814,-0.8951747880199079,null,0.00531037562885814,0.6956510259911269,null,0.00531037562885814,-0.5171428508342801,null,0.00531037562885814,-0.22352731242200344,null,0.00531037562885814,0.721110221990472,null,0.00531037562885814,0.5013988043952683,null,0.00531037562885814,-0.6522612595637475,null,0.00531037562885814,-0.8725383044493782,null,0.00531037562885814,0.02284949484742228,null,0.00531037562885814,0.9605275655528687,null,0.00531037562885814,0.25379702688330086,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"lightblue\",\"size\":10,\"symbol\":\"circle\"},\"mode\":\"markers\",\"text\":[\"<b>LinkBERT: Pretraining Language Models with Document Links</b><br>Michihiro Yasunaga, Jure Leskovec, Percy Liang<br>2203.15827\",\"<b>HTLM: Hyper-Text Pre-Training and Prompting of Language Models</b><br>Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, Luke Zettlemoyer<br>2107.06955\",\"<b>On the Opportunities and Risks of Foundation Models</b><br>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang<br>2108.07258\",\"<b>CDLM: Cross-Document Language Modeling</b><br>Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie Cattan, Ido Dagan<br>2101.00406\",\"<b>SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine</b><br>Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, Kyunghyun Cho<br>1704.05179\",\"<b>Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</b><br>Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon<br>2007.15779\",\"<b>The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design</b><br>Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua<br>2110.04541\",\"<b>RoBERTa: A Robustly Optimized BERT Pretraining Approach</b><br>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov<br>1907.11692\",\"<b>UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering</b><br>Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Schlichtkrull, Sonal Gupta, Yashar Mehdad, Scott Yih<br>2012.14610\",\"<b>Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</b><br>Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova<br>1908.08962\",\"<b>UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</b><br>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu<br>2201.05966\",\"<b>ERNIE: Enhanced Language Representation with Informative Entities</b><br>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu<br>1905.07129\"],\"x\":[0.0056691370406218005,0.34704563667013444,-0.6615856134080516,0.78552175504422,-0.8600272204102756,0.7364944133101362,0.5584055319426203,-0.6311101411542009,-0.17241632276139432,0.9999999999999999,-0.20511849744920788,-0.9028786788246024],\"y\":[0.00531037562885814,-0.8951747880199079,0.6956510259911269,-0.5171428508342801,-0.22352731242200344,0.721110221990472,0.5013988043952683,-0.6522612595637475,-0.8725383044493782,0.02284949484742228,0.9605275655528687,0.25379702688330086],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Graphique des citations\",\"x\":0.5,\"font\":{\"size\":20}},\"width\":800,\"height\":600,\"showlegend\":false,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('3bcc8a50-18de-4738-ae23-8f81c66f57f4');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"cell_id":"f68aaf9c410a425f9ed3142ae9b2b6f6","deepnote_cell_type":"markdown"},"source":"# Test sur un mini corpus ","block_group":"f68aaf9c410a425f9ed3142ae9b2b6f6"},{"cell_type":"code","metadata":{"cell_id":"d363c76a69d341738081aba8a97805fd","source_hash":"22f4aab7","output_cleared":false,"execution_start":1684220897447,"execution_millis":8311,"is_output_hidden":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import os\n\n# Chemin vers le dossier contenant les PDF\npdf_folder = '/work/Test'\n\n# Liste pour stocker les graphes de citation\ncitation_graphs = []\n\n# Boucle à travers les fichiers PDF dans le dossier\nfor filename in os.listdir(pdf_folder):\n    if filename.endswith('.pdf'):\n        # Chemin complet vers le fichier PDF\n        filepath = os.path.join(pdf_folder, filename)\n\n        # Appeler la fonction pour générer le graphe de citation\n        citation_graph = process_citation_graph(filepath)\n\n        # Ajouter le graphe de citation à la liste\n        citation_graphs.append(citation_graph)\n","block_group":"d363c76a69d341738081aba8a97805fd","execution_count":19,"outputs":[{"data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"a2cdf1a9-3b26-4503-a985-4b94b2a49f5a\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a2cdf1a9-3b26-4503-a985-4b94b2a49f5a\")) {                    Plotly.newPlot(                        \"a2cdf1a9-3b26-4503-a985-4b94b2a49f5a\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"gray\",\"width\":0.5},\"mode\":\"lines\",\"x\":[-0.014337870130313563,-0.10331097866180544,null,-0.014337870130313563,-0.39523431683373694,null,-0.014337870130313563,0.7268010055060793,null,-0.014337870130313563,-1.0,null,-0.014337870130313563,0.7860821601197766,null],\"y\":[-0.016045815014267432,0.9711114114953833,null,-0.016045815014267432,-0.8930282808594004,null,-0.016045815014267432,0.3545931197968832,null,-0.016045815014267432,0.3690597530759062,null,-0.016045815014267432,-0.7856901884945047,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"lightblue\",\"size\":10,\"symbol\":\"circle\"},\"mode\":\"markers\",\"text\":[\"<b>TactoFind: A Tactile Only System for Object Retrieval</b><br>Sameer Pai, Tao Chen, Megha Tippur, Edward Adelson, Abhishek Gupta, Pulkit Agrawal<br>2303.13482\",\"<b>ReSkin: versatile, replaceable, lasting tactile skins</b><br>Raunaq Bhirangi, Tess Hellebrekers, Carmel Majidi, Abhinav Gupta<br>2111.00071\",\"<b>TANDEM: Learning Joint Exploration and Decision Making with Tactile Sensors</b><br>Jingxi Xu, Shuran Song, Matei Ciocarlie<br>2203.00798\",\"<b>Visual Dexterity: In-hand Dexterous Manipulation from Depth</b><br>Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, Pulkit Agrawal<br>2211.11744\",\"<b>Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items</b><br>Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, Vincent Vanhoucke<br>2204.11918\",\"<b>ShapeNet: An Information-Rich 3D Model Repository</b><br>Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu<br>1512.03012\"],\"x\":[-0.014337870130313563,-0.10331097866180544,-0.39523431683373694,0.7268010055060793,-1.0,0.7860821601197766],\"y\":[-0.016045815014267432,0.9711114114953833,-0.8930282808594004,0.3545931197968832,0.3690597530759062,-0.7856901884945047],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Graphique des citations\",\"x\":0.5,\"font\":{\"size\":20}},\"width\":800,\"height\":600,\"showlegend\":false,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('a2cdf1a9-3b26-4503-a985-4b94b2a49f5a');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"c574ccbe-4977-40c6-85e7-8562429c915b\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c574ccbe-4977-40c6-85e7-8562429c915b\")) {                    Plotly.newPlot(                        \"c574ccbe-4977-40c6-85e7-8562429c915b\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"gray\",\"width\":0.5},\"mode\":\"lines\",\"x\":[-0.022492089436586038,-0.975123319086285,null,-0.022492089436586038,0.5348719245306204,null,-0.022492089436586038,-0.38434328526690087,null,-0.022492089436586038,0.24511492415723285,null,-0.022492089436586038,0.6019718451019187,null],\"y\":[0.0200209026110959,-0.0027409148794305233,null,0.0200209026110959,-0.665427195354742,null,0.0200209026110959,-0.7367989143768756,null,0.0200209026110959,1.0,null,0.0200209026110959,0.3849461219999521,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"lightblue\",\"size\":10,\"symbol\":\"circle\"},\"mode\":\"markers\",\"text\":[\"<b>On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and Learning</b><br>Maks Sorokin, Chuyuan Fu, Jie Tan, C. Karen Liu, Yunfei Bai, Wenlong Lu, Sehoon Ha, Mohi Khansari<br>2303.13390\",\"<b>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</b><br>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng<br>2204.01691\",\"<b>RT-1: Robotics Transformer for Real-World Control at Scale</b><br>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich<br>2212.06817\",\"<b>Transformers Meet Visual Learning Understanding: A Comprehensive Review</b><br>Yuting Yang, Licheng Jiao, Xu Liu, Fang Liu, Shuyuan Yang, Zhixi Feng, Xu Tang<br>2010.11929\",\"<b>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</b><br>Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine<br>1806.10293\",\"<b>ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for Mobile Manipulation</b><br>Fei Xia, Chengshu Li, Roberto Mart\\u00edn-Mart\\u00edn, Or Litany, Alexander Toshev, Silvio Savarese<br>2008.07792\"],\"x\":[-0.022492089436586038,-0.975123319086285,0.5348719245306204,-0.38434328526690087,0.24511492415723285,0.6019718451019187],\"y\":[0.0200209026110959,-0.0027409148794305233,-0.665427195354742,-0.7367989143768756,1.0,0.3849461219999521],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Graphique des citations\",\"x\":0.5,\"font\":{\"size\":20}},\"width\":800,\"height\":600,\"showlegend\":false,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('c574ccbe-4977-40c6-85e7-8562429c915b');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"367c463d-b7b0-4503-8645-f411cb35c493\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"367c463d-b7b0-4503-8645-f411cb35c493\")) {                    Plotly.newPlot(                        \"367c463d-b7b0-4503-8645-f411cb35c493\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"gray\",\"width\":0.5},\"mode\":\"lines\",\"x\":[0.006876121560117489,0.8937893808324474,null,0.006876121560117489,-0.9754703589387166,null,0.006876121560117489,0.4341107379158822,null,0.006876121560117489,0.21325122107169836,null,0.006876121560117489,1.0,null,0.006876121560117489,-0.4655510810419191,null,0.006876121560117489,-0.25805921123813924,null,0.006876121560117489,-0.8489468101613706,null],\"y\":[0.0017809053816050609,0.5517853819188524,null,0.0017809053816050609,0.24373594858270767,null,0.0017809053816050609,-0.8248721312973655,null,0.0017809053816050609,0.813307530503903,null,0.0017809053816050609,-0.2714065252703302,null,0.0017809053816050609,0.810745213012465,null,0.0017809053816050609,-0.8111571680604072,null,0.0017809053816050609,-0.5139191547714304,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"lightblue\",\"size\":10,\"symbol\":\"circle\"},\"mode\":\"markers\",\"text\":[\"<b>On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills</b><br>Yunhai Han, Mandy Xie, Ye Zhao, Harish Ravichandar<br>2303.13446\",\"<b>In-Hand Object Rotation via Rapid Motor Adaptation</b><br>Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, Jitendra Malik<br>2210.04887\",\"<b>Holo-Dex: Teaching Dexterity with Immersive Mixed Reality</b><br>Sridhar Pandian Arunachalam, Irmak G\\u00fczey, Soumith Chintala, Lerrel Pinto<br>2210.06463\",\"<b>Learning Dexterous Manipulation Policies from Experience and Imitation</b><br>Vikash Kumar, Abhishek Gupta, Emanuel Todorov, Sergey Levine<br>1611.05095\",\"<b>Model-Based Control Using Koopman Operators</b><br>Ian Abraham, Gerardo De La Torre, Todd D. Murphey<br>1709.01568\",\"<b>Modeling and Control of Soft Robots Using the Koopman Operator and Model Predictive Control</b><br>Daniel Bruder, Brent Gillespie, C. David Remy, Ram Vasudevan<br>1902.02827\",\"<b>Learning Agile Robotic Locomotion Skills by Imitating Animals</b><br>Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, Sergey Levine<br>2004.00784\",\"<b>Recent Advances in Imitation Learning from Observation</b><br>Faraz Torabi, Garrett Warnell, Peter Stone<br>1905.13566\",\"<b>What Matters in Learning from Offline Human Demonstrations for Robot Manipulation</b><br>Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Mart\\u00edn-Mart\\u00edn<br>2108.03298\"],\"x\":[0.006876121560117489,0.8937893808324474,-0.9754703589387166,0.4341107379158822,0.21325122107169836,1.0,-0.4655510810419191,-0.25805921123813924,-0.8489468101613706],\"y\":[0.0017809053816050609,0.5517853819188524,0.24373594858270767,-0.8248721312973655,0.813307530503903,-0.2714065252703302,0.810745213012465,-0.8111571680604072,-0.5139191547714304],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Graphique des citations\",\"x\":0.5,\"font\":{\"size\":20}},\"width\":800,\"height\":600,\"showlegend\":false,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('367c463d-b7b0-4503-8645-f411cb35c493');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"93f17854-8e0b-4acb-9ac1-7c4e05176334\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"93f17854-8e0b-4acb-9ac1-7c4e05176334\")) {                    Plotly.newPlot(                        \"93f17854-8e0b-4acb-9ac1-7c4e05176334\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"gray\",\"width\":0.5},\"mode\":\"lines\",\"x\":[-0.005845702631377002,-0.2213430672402088,null,-0.005845702631377002,-0.9748104121937575,null,-0.005845702631377002,0.3481568202232287,null,-0.005845702631377002,0.8538423618421145,null],\"y\":[0.021475732417078088,-0.9128963898157108,null,0.021475732417078088,0.2252019493403029,null,0.021475732417078088,1.0,null,0.021475732417078088,-0.3337812919416702,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"lightblue\",\"size\":10,\"symbol\":\"circle\"},\"mode\":\"markers\",\"text\":[\"<b>Covariance Steering for Uncertain Contact-rich Systems</b><br>Yuki Shirai, Devesh K. Jha, Arvind U. Raghunathan<br>2303.13382\",\"<b>Chance-Constrained Optimization in Contact-Rich Systems for Robust Manipulation</b><br>Yuki Shirai, Devesh K. Jha, Arvind Raghunathan, Diego Romeres<br>2203.02616\",\"<b>Trajectory Optimization of Chance-Constrained Nonlinear Stochastic Systems for Motion Planning Under Uncertainty</b><br>Yashwanth Kumar Nakka, Soon-Jo Chung<br>2106.02801\",\"<b>Fast Risk Assessment for Autonomous Vehicles Using Learned Models of Agent Futures</b><br>Allen Wang, Xin Huang, Ashkan Jasour, Brian Williams<br>2005.13458\",\"<b>Moment-Based Exact Uncertainty Propagation Through Nonlinear Stochastic Autonomous Systems</b><br>Ashkan Jasour, Allen Wang, Brian C. Williams<br>2101.12490\"],\"x\":[-0.005845702631377002,-0.2213430672402088,-0.9748104121937575,0.3481568202232287,0.8538423618421145],\"y\":[0.021475732417078088,-0.9128963898157108,0.2252019493403029,1.0,-0.3337812919416702],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Graphique des citations\",\"x\":0.5,\"font\":{\"size\":20}},\"width\":800,\"height\":600,\"showlegend\":false,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('93f17854-8e0b-4acb-9ac1-7c4e05176334');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=61df9ec0-4810-4690-90cf-bbe4fbfededd' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"23d4de50a4054dc1812d09ba61d8d56e","deepnote_execution_queue":[]}}